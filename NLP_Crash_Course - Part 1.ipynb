{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Crash Course - Part 1: Data Preparation\n",
    "Prepared by: Nickolas Freeman, PhD\n",
    "\n",
    "In this notebook, we will begin looking at how we can use Python for Natural Language Processing (NLP). From https://en.wikipedia.org/wiki/Natural_language_processing:\n",
    "> Natural language processing (NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "The following code block imports some libraries that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries that are part of the Python standard library\n",
    "import gzip\n",
    "import itertools\n",
    "import json\n",
    "import pathlib\n",
    "import string\n",
    "import time\n",
    "\n",
    "# Third-party packages\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with almost all analytical efforts, the quality of a model or application will be largely dependent on the quality of the data used for the analysis. Thus, the focus of this notebook will be on demonstrating how we can use the **Natural Language Toolkit (NLTK)** to prepare a *corpus* of text data for analysis. From https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html:\n",
    "> One of the first things required for natural language processing (NLP) tasks is a corpus. In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts.\n",
    "\n",
    "We will be using the `nltk` library for our data preparation. If you have not worked with `nltk` before, you will need to run the `nltk.download()` function to download additional resources used by the library. You can do this using the following code block by setting the value of the `run_nltk_downloader` variable to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nltk_downloader = False\n",
    "\n",
    "if run_nltk_downloader:\n",
    "    nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we will be using was harvested from the twitter API (see https://developer.twitter.com/en/docs for more information). The data is stored in JSON (JavaScript Object Notation) format. The JSON file is compressed using GZIP compression (see https://en.wikipedia.org/wiki/Gzip for more information) to save disk space. The following code block demonstrates how to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data includes 434578 observations.\n"
     ]
    }
   ],
   "source": [
    "# create a pathlib.Path object for the data\n",
    "raw_data_filepath = pathlib.Path('relevant_data.json.gz')\n",
    "\n",
    "# open the data filepath\n",
    "with gzip.GzipFile(raw_data_filepath, 'r') as fp:\n",
    "    \n",
    "    # read the bytes from the datafile\n",
    "    json_bytes = fp.read()\n",
    "    \n",
    "    # decode the bytes to utf-8\n",
    "    json_str = json_bytes.decode('utf-8')\n",
    "    \n",
    "    # load the decoded bytes as JSON\n",
    "    tweet_data = json.loads(json_str)\n",
    "    \n",
    "print(f'The data includes {len(tweet_data)} observations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored as a list of lists. Each sublist has six items:\n",
    "1. the datetime for the tweet,\n",
    "2. the username associated with the tweet,\n",
    "3. the user associated with the tweet,\n",
    "4. the location associated with the tweet,\n",
    "5. the language associated with the tweet, and\n",
    "6. the tweet text.\n",
    "\n",
    "The following code block prints the first entry as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sat Mar 28 08:21:12 +0000 2020', 'DianeB67082104', 'Diane B', 'Yorkshire and The Humber', 'en', '@redviking_82 Yea we are fine staying home as much as we can Hubby has to go to work (keyworker food production) and we shop for his parents I stay in the car. His Mum likes to see me ❤️ Stay safe love xx']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block:\n",
    "1. creates a Pandas `DataFrame` with the data (named `tweet_df`),\n",
    "2. converts the `Datetime` column to a datetime format,\n",
    "3. creates a `Date` column with just the date extracted from the `Datetime` column,\n",
    "4. creates a `Day_Name` column that specifies the day of the week that the tweet was posted.\n",
    "5. creates a `Weekend` column that is `True` if the tweet was posted on a weekend, and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Username</th>\n",
       "      <th>User</th>\n",
       "      <th>Location</th>\n",
       "      <th>Language</th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day_Name</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-28 08:21:12+00:00</td>\n",
       "      <td>DianeB67082104</td>\n",
       "      <td>Diane B</td>\n",
       "      <td>Yorkshire and The Humber</td>\n",
       "      <td>en</td>\n",
       "      <td>@redviking_82 Yea we are fine staying home as ...</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-28 09:07:14+00:00</td>\n",
       "      <td>BiotechPolicyUK</td>\n",
       "      <td>Michael M. Hopkins</td>\n",
       "      <td>Brighton, UK</td>\n",
       "      <td>en</td>\n",
       "      <td>This is a helpful analysis of NERVTAG delibera...</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-28 00:14:05+00:00</td>\n",
       "      <td>MarketingEdgeM</td>\n",
       "      <td>Marketing Edge</td>\n",
       "      <td>Lagos, Nigeria</td>\n",
       "      <td>en</td>\n",
       "      <td>Kantar, the world’s leading data, insights and...</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-28 05:36:36+00:00</td>\n",
       "      <td>Mrtroublelex</td>\n",
       "      <td>Raynomics</td>\n",
       "      <td>Warri, Nigeria</td>\n",
       "      <td>en</td>\n",
       "      <td>@iRuntown__ I been like you well well, but all...</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-28 11:10:05+00:00</td>\n",
       "      <td>whitedragoncom</td>\n",
       "      <td>Kin Dle</td>\n",
       "      <td>None</td>\n",
       "      <td>en</td>\n",
       "      <td>Jesus Christ Christian Bible Prophecy News - F...</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Datetime         Username                User  \\\n",
       "0 2020-03-28 08:21:12+00:00   DianeB67082104             Diane B   \n",
       "1 2020-03-28 09:07:14+00:00  BiotechPolicyUK  Michael M. Hopkins   \n",
       "2 2020-03-28 00:14:05+00:00   MarketingEdgeM      Marketing Edge   \n",
       "3 2020-03-28 05:36:36+00:00     Mrtroublelex           Raynomics   \n",
       "4 2020-03-28 11:10:05+00:00   whitedragoncom             Kin Dle   \n",
       "\n",
       "                   Location Language  \\\n",
       "0  Yorkshire and The Humber       en   \n",
       "1              Brighton, UK       en   \n",
       "2            Lagos, Nigeria       en   \n",
       "3            Warri, Nigeria       en   \n",
       "4                      None       en   \n",
       "\n",
       "                                                Text        Date  Day_Name  \\\n",
       "0  @redviking_82 Yea we are fine staying home as ...  2020-03-28  Saturday   \n",
       "1  This is a helpful analysis of NERVTAG delibera...  2020-03-28  Saturday   \n",
       "2  Kantar, the world’s leading data, insights and...  2020-03-28  Saturday   \n",
       "3  @iRuntown__ I been like you well well, but all...  2020-03-28  Saturday   \n",
       "4  Jesus Christ Christian Bible Prophecy News - F...  2020-03-28  Saturday   \n",
       "\n",
       "  Weekend  Hour  \n",
       "0    True     8  \n",
       "1    True     9  \n",
       "2    True     0  \n",
       "3    True     5  \n",
       "4    True    11  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [\n",
    "    'Datetime',\n",
    "    'Username',\n",
    "    'User',\n",
    "    'Location',\n",
    "    'Language',\n",
    "    'Text',\n",
    "]\n",
    "\n",
    "tweet_df = pd.DataFrame(tweet_data, \n",
    "                        columns = column_names)\n",
    "\n",
    "tweet_df['Datetime'] = pd.to_datetime(tweet_df['Datetime'], \n",
    "                                      format = '%a %b %d %H:%M:%S %z %Y')\n",
    "tweet_df['Date'] = tweet_df['Datetime'].dt.date\n",
    "tweet_df['Day_Name'] = tweet_df['Datetime'].dt.day_name()\n",
    "\n",
    "weekend_mask = tweet_df['Day_Name'].isin(['Saturday', 'Sunday'])\n",
    "tweet_df.loc[weekend_mask, 'Weekend'] = True\n",
    "tweet_df.loc[~weekend_mask, 'Weekend'] = False\n",
    "\n",
    "tweet_df['Hour'] = tweet_df['Datetime'].dt.hour\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier code block shows that our data includes hundreds of thousands of tweets. For the sake of time, we will only work with a random sample of these tweets when demonstrating the data preparation steps. The following code block shows how we can use `numpy` to obtain a random sample of the tweets. **Note:** Specifying the `seed` for the random number generator allows us to draw *consistent* random samples. The first tweet in the sample is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@olandgren Dementia means my Dad has to listen to my Mother's questions all day about what is happening. She is still confused about COVID. She certainly does not understand the rioting.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "sample_size = 1000\n",
    "sample = np.random.choice(tweet_df['Text'].tolist(), \n",
    "                          size = sample_size,\n",
    "                          replace = False,\n",
    "                         )\n",
    "sample = sample.tolist()\n",
    "print(sample[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common first step in data preparation for NLP is tokenization. From https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/:\n",
    "> Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.\n",
    "\n",
    "In this notebook, we will be focusing on tokenizing the tweets into words or phrases (n-grams). We will begin with word tokenization. A obvious approach to such tokenization is splitting texts (tweets in our case) by spaces. The following code block shows how we can do this for the first tweet using Python's builtin `split` method for strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@olandgren', 'Dementia', 'means', 'my', 'Dad', 'has', 'to', 'listen', 'to', 'my', \"Mother's\", 'questions', 'all', 'day', 'about', 'what', 'is', 'happening.', 'She', 'is', 'still', 'confused', 'about', 'COVID.', 'She', 'certainly', 'does', 'not', 'understand', 'the', 'rioting.']\n"
     ]
    }
   ],
   "source": [
    "current_tweet = sample[0]\n",
    "print(current_tweet.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple tokenization method of splitting on spaces works perfectly for the first tweet. However, let's consider another example where the tweet includes emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@chrissyteigen', 'I’m', 'a', 'Native', 'American', 'living', 'on', 'a', 'reservation', 'in', '#Manitoba', '#Indigenous', 'we', 'love', 'you', '😍', '❤️🇨🇦👩🏽\\u200d🍳👩🏽\\u200d🍳👩🏽\\u200d🍳👩🏽\\u200d🍳👩🏽\\u200d🍳👩🏽\\u200d🍳🌹#StaySafeStayHome', '#food', '#TopChef']\n"
     ]
    }
   ],
   "source": [
    "current_tweet = sample[2]\n",
    "print(current_tweet.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our space-splitting method does not perform as well in this case. This is where the `nltk` library can be helpful. In particular, the library includes several *tokenizers* that can be employed and customized to improve tokenization for a given task. The following code block shows the various tokenizers that are available in the `nltk.tokenize` module. Of particular interest for us, note that the module includes a `TweetTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BlanklineTokenizer', 'LineTokenizer', 'MWETokenizer', 'NLTKWordTokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'SyllableTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'destructive', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'sonority_sequencing', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']\n"
     ]
    }
   ],
   "source": [
    "print(dir(nltk.tokenize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can use the `TweetTokenizer` to tokenize the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@chrissyteigen', 'I', '’', 'm', 'a', 'Native', 'American', 'living', 'on', 'a', 'reservation', 'in', '#Manitoba', '#Indigenous', 'we', 'love', 'you', '😍', '❤', '️', '🇨', '🇦', '👩', '🏽', '\\u200d', '🍳', '👩', '🏽', '\\u200d', '🍳', '👩', '🏽', '\\u200d', '🍳', '👩', '🏽', '\\u200d', '🍳', '👩', '🏽', '\\u200d', '🍳', '👩', '🏽', '\\u200d', '🍳', '🌹', '#StaySafeStayHome', '#food', '#TopChef']\n"
     ]
    }
   ],
   "source": [
    "current_tweet = sample[2]\n",
    "tknzr = nltk.tokenize.TweetTokenizer()\n",
    "print(tknzr.tokenize(current_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the `TweetTokenizer` seems to do a pretty good job. Let's consider another tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet:\n",
      "\n",
      "\"New Article: Coronavirus: Food market shut in Beijing after 45 cases; hospitalizations rise in some U.S. states https://t.co/qH69klFXxw #businessNews June 13, 2020\"\n",
      "\n",
      "is tokenized as:\n",
      "\n",
      " ['New', 'Article', ':', 'Coronavirus', ':', 'Food', 'market', 'shut', 'in', 'Beijing', 'after', '45', 'cases', ';', 'hospitalizations', 'rise', 'in', 'some', 'U', '.', 'S', '.', 'states', 'https://t.co/qH69klFXxw', '#businessNews', 'June', '13', ',', '2020']\n"
     ]
    }
   ],
   "source": [
    "current_tweet = sample[266]\n",
    "tknzr = nltk.tokenize.TweetTokenizer()\n",
    "print(f'The tweet:\\n\\n\"{current_tweet}\"\\n\\nis tokenized as:\\n\\n {tknzr.tokenize(current_tweet)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this last tweet, the term *U.S.* is tokenized into four tokens:\n",
    "```python\n",
    "['U', '.', 'S', '.']\n",
    "```\n",
    "\n",
    "If there are certain *multi-word* expressions that we wish to retain, we can use the *multi-word* tokenizer (`MWETokenizer`) along with the `TweetTokenizer`. The following code block shows how this can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New',\n",
       " 'Article',\n",
       " ':',\n",
       " 'Coronavirus',\n",
       " ':',\n",
       " 'Food',\n",
       " 'market',\n",
       " 'shut',\n",
       " 'in',\n",
       " 'Beijing',\n",
       " 'after',\n",
       " '45',\n",
       " 'cases',\n",
       " ';',\n",
       " 'hospitalizations',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'some',\n",
       " 'U_._S_.',\n",
       " 'states',\n",
       " 'https://t.co/qH69klFXxw',\n",
       " '#businessNews',\n",
       " 'June',\n",
       " '13',\n",
       " ',',\n",
       " '2020']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate an instance of the multi-word tokenizer\n",
    "mwe_tknzr = nltk.tokenize.MWETokenizer()\n",
    "\n",
    "# add a pattern that we want to retain\n",
    "mwe_tknzr.add_mwe(('U', '.', 'S', '.'))\n",
    "\n",
    "# pass a list of tokens to the multi-word tokenizer\n",
    "mwe_tknzr.tokenize(tknzr.tokenize(current_tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can apply the `TweetTokenizer` and `MWETokenizer` to all of the tweets in our sample, storing the the tokens for each tweet in a list object named `tweet_tokens`. **Note** that applying the tokenizer to a single tweet returned a list of all the tokens in the tweet. Thus, our `tweet_tokens` object will be a list of lists, where each sublist specifies the tokens for a single tweet.\n",
    "\n",
    "After constructing the `tweet_tokens` object, we construct a list of all tokens and use the `nltk.FreqDist` function to identify the 30 tokens that occur most frequently in the sample of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 1491),\n",
       " ('the', 1147),\n",
       " (',', 1043),\n",
       " ('to', 959),\n",
       " ('and', 830),\n",
       " ('of', 655),\n",
       " ('a', 554),\n",
       " ('in', 477),\n",
       " ('is', 427),\n",
       " ('for', 393),\n",
       " ('-', 388),\n",
       " ('I', 343),\n",
       " ('’', 333),\n",
       " ('19', 306),\n",
       " ('are', 302),\n",
       " ('COVID', 289),\n",
       " ('!', 280),\n",
       " ('you', 265),\n",
       " ('it', 255),\n",
       " ('that', 251),\n",
       " ('?', 247),\n",
       " ('food', 246),\n",
       " ('on', 225),\n",
       " ('with', 220),\n",
       " ('have', 210),\n",
       " ('this', 200),\n",
       " ('not', 185),\n",
       " (':', 172),\n",
       " ('Covid', 171),\n",
       " ('be', 169)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokens = [tknzr.tokenize(current_tweet) for current_tweet in sample]\n",
    "tweet_tokens = [mwe_tknzr.tokenize(current_tweet) for current_tweet in tweet_tokens]\n",
    "all_tokens = list(itertools.chain.from_iterable(tweet_tokens))\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "freq_dist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the most common 30 tokens reveals some potential issues. First, many of the most common tokens are punctuation symbols. Second, we observe some inconsistencies due to spelling case, e.g., COVID vs. Covid. Third, we observe many words that may not necessarily be too informative with regard to understanding a particular text, e.g., the, and, etc. \n",
    "\n",
    "We will now work to handle these issues, starting with punctuation. For this purpose, we will use the `string` library. The following code block shows how we can use the library to get a string that contains common punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can use the `in` comparison operator to see if a specific character is in the common punctuation string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `nltk` library includes a list of stopwords that we can use to remove common words that add little additional information to texts. The following code block shows how we can access the list of common stopwords for the English language using `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did for punctuation, we can use the `in` comparison operator to see if a string is a stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'the' in nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block contains a revised tokenization process that handles the three previously identified issues, using the builtin `lower` method for strings to handle the spelling case issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process took 10.75 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid', 565),\n",
       " ('’', 333),\n",
       " ('19', 306),\n",
       " ('food', 278),\n",
       " ('people', 130),\n",
       " ('...', 128),\n",
       " ('get', 93),\n",
       " ('like', 82),\n",
       " ('us', 82),\n",
       " ('one', 60),\n",
       " ('help', 59),\n",
       " ('time', 55),\n",
       " ('even', 54),\n",
       " ('pandemic', 54),\n",
       " ('deaths', 54),\n",
       " ('still', 53),\n",
       " ('..', 51),\n",
       " ('think', 50),\n",
       " ('need', 49),\n",
       " ('trump', 49),\n",
       " ('would', 48),\n",
       " ('health', 47),\n",
       " ('#covid19', 47),\n",
       " (':/', 47),\n",
       " ('good', 46),\n",
       " ('new', 45),\n",
       " ('many', 44),\n",
       " ('see', 44),\n",
       " ('virus', 44),\n",
       " ('cases', 43)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# update mwe tokenizer (note that we are using lowercase)\n",
    "mwe_tknzr.add_mwe(('u', '.', 's', '.'))\n",
    "\n",
    "tweet_tokens = []\n",
    "for current_tweet in sample:\n",
    "    # tokenize a lower-cased version of the tweet\n",
    "    current_tokens = tknzr.tokenize(current_tweet.lower())\n",
    "    \n",
    "    # apply multi-word expression tokenizer\n",
    "    current_tokens = mwe_tknzr.tokenize(current_tokens)\n",
    "    \n",
    "    # remove puntuation\n",
    "    current_tokens = [current_token for current_token in current_tokens if current_token not in string.punctuation]\n",
    "    \n",
    "    # remove stopwords\n",
    "    current_tokens = [current_token for current_token in current_tokens if current_token not in nltk.corpus.stopwords.words('english')]\n",
    "    tweet_tokens.append(current_tokens)\n",
    "    \n",
    "all_tokens = list(itertools.chain.from_iterable(tweet_tokens))\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "end_time = time.time()\n",
    "print(f'Process took {round(end_time - start_time, 2)} seconds.')\n",
    "\n",
    "freq_dist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is better, we can still make some improvements by defining custom stopwords to catch some of the less traditional strings we observe. This is done in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process took 0.43 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid', 565),\n",
       " ('19', 306),\n",
       " ('food', 278),\n",
       " ('people', 130),\n",
       " ('like', 82),\n",
       " ('us', 82),\n",
       " ('one', 60),\n",
       " ('help', 59),\n",
       " ('time', 55),\n",
       " ('even', 54),\n",
       " ('pandemic', 54),\n",
       " ('deaths', 54),\n",
       " ('still', 53),\n",
       " ('think', 50),\n",
       " ('need', 49),\n",
       " ('trump', 49),\n",
       " ('would', 48),\n",
       " ('health', 47),\n",
       " ('#covid19', 47),\n",
       " (':/', 47),\n",
       " ('good', 46),\n",
       " ('new', 45),\n",
       " ('many', 44),\n",
       " ('see', 44),\n",
       " ('virus', 44),\n",
       " ('cases', 43),\n",
       " ('please', 42),\n",
       " ('know', 42),\n",
       " ('home', 41),\n",
       " (\"i'm\", 41)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "my_stopwords.extend(['..', '...', 'get', '\\u200d', '’', '“', '”'])\n",
    "\n",
    "tweet_tokens = []\n",
    "for current_tweet in sample:\n",
    "    current_tokens = tknzr.tokenize(current_tweet.lower())\n",
    "    current_tokens = mwe_tknzr.tokenize(current_tokens)\n",
    "    current_tokens = [current_token for current_token in current_tokens if current_token not in string.punctuation]\n",
    "    current_tokens = [current_token for current_token in current_tokens if current_token not in my_stopwords]\n",
    "    tweet_tokens.append(current_tokens)\n",
    "    \n",
    "all_tokens = list(itertools.chain.from_iterable(tweet_tokens))\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "end_time = time.time()\n",
    "print(f'Process took {round(end_time - start_time, 2)} seconds.')\n",
    "\n",
    "freq_dist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last list looks better. **By the way, did you notice the speed difference between the previous two code blocks? What is happening?**\n",
    "\n",
    "Although it is not evident in the previous list of common tokens, there is another issue that might cause some problems. In particular, notice that the list includes the words *deaths* and *cases*, which are the plural forms of *death* and *case*. Should we consider the singular and plural forms of words as different words? Typically, we use a technique called lemmatization to handle such cases. From https://en.wikipedia.org/wiki/Lemmatisation:\n",
    "> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "Again, `nltk` comes to the rescue with several *lemmatizers* built in. The following code block shows how we can instantiate a lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block shows how we can use the `lemmatize` method of the instantiated lemmatizer to lemmatize the first tweet in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: @olandgren -> @olandgren\n",
      "Token: dementia -> dementia\n",
      "Token: means -> mean\n",
      "Token: dad -> dad\n",
      "Token: listen -> listen\n",
      "Token: mother's -> mother's\n",
      "Token: questions -> question\n",
      "Token: day -> day\n",
      "Token: happening -> happening\n",
      "Token: still -> still\n",
      "Token: confused -> confused\n",
      "Token: covid -> covid\n",
      "Token: certainly -> certainly\n",
      "Token: understand -> understand\n",
      "Token: rioting -> rioting\n"
     ]
    }
   ],
   "source": [
    "for token in tweet_tokens[0]:\n",
    "    print(f'Token: {token} -> {lemmatizer.lemmatize(token)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go further by reducing lemmatized tokens to their respective stems. From https://en.wikipedia.org/wiki/Stemming:\n",
    ">stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n",
    "\n",
    "Again, `nltk` has a variety of builtin *stemmers*. The following code block instantiates a popular stemmer, the *Porter* stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block applies the stemmer to our lemmatized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: @olandgren -> @olandgren -> @olandgren\n",
      "Token: dementia -> dementia -> dementia\n",
      "Token: means -> mean -> mean\n",
      "Token: dad -> dad -> dad\n",
      "Token: listen -> listen -> listen\n",
      "Token: mother's -> mother's -> mother'\n",
      "Token: questions -> question -> question\n",
      "Token: day -> day -> day\n",
      "Token: happening -> happening -> happen\n",
      "Token: still -> still -> still\n",
      "Token: confused -> confused -> confus\n",
      "Token: covid -> covid -> covid\n",
      "Token: certainly -> certainly -> certainli\n",
      "Token: understand -> understand -> understand\n",
      "Token: rioting -> rioting -> riot\n"
     ]
    }
   ],
   "source": [
    "for token in tweet_tokens[0]:\n",
    "    lemma = lemmatizer.lemmatize(token)\n",
    "    stem = stemmer.stem(lemma)\n",
    "    print(f'Token: {token} -> {lemma} -> {stem}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function that we can use to apply all of our data preparation techniques a list of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_texts(list_of_texts, \n",
    "                  mwe_tuples_list = None, \n",
    "                  custom_stopword_list = None,\n",
    "                  lemmatize = True,\n",
    "                  stem = True,\n",
    "                  remove_punctuation = True,\n",
    "                  remove_stopwords = True,\n",
    "                 ):\n",
    "    '''\n",
    "    This function accepts a list of texts, which are assumed to be tweets. \n",
    "    For each text in the list:\n",
    "    1. the TweetTokenizer is used to tokenize the texts,\n",
    "    2. detects the multi-word expressions specified in the mwe_tuples_list,\n",
    "    3. removes tokens corresponding to common punctuation symbols,\n",
    "    4. removes tokens corresponding to common stopwords and custom stopwords\n",
    "        defined in the custom_stopword_list,\n",
    "    5. lemmatizes the tokens using the WordNetLemmatizer, and\n",
    "    6. stems the tokens using the PorterStemmer.\n",
    "    \n",
    "    The function also allows you to specify flags to control the text preparation\n",
    "    process. In particular, the flags:\n",
    "    lemmatize - controls whether or not tokens are lemmatized (default = True)\n",
    "    stem - controls whether or not tokens are stemmed (default = True)\n",
    "    remove_punctuation - controls whether or not punctuation is removed (default = True)\n",
    "    remove_stopwords - controls whether or not stopwords are removed (default = True)\n",
    "    \n",
    "    No flag is included for multi-word expressions. To skip this aspect, simply\n",
    "    do not provide a list of multi-word expression tuples.\n",
    "    \n",
    "    The function returns a list of list. Each sublist contains the tokens\n",
    "    for each of the texts in the original list_of_texts.\n",
    "    \n",
    "    '''\n",
    "    tknzr = nltk.tokenize.TweetTokenizer()\n",
    "    \n",
    "    mwe_tknzr = nltk.tokenize.MWETokenizer()\n",
    "    if mwe_tuples_list is not None:\n",
    "        for mwe_tuple in mwe_tuples_list:\n",
    "            mwe_tknzr.add_mwe(mwe_tuple)\n",
    "            \n",
    "    if remove_stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        if custom_stopword_list is not None:\n",
    "            stopwords.extend(custom_stopword_list)\n",
    "    if lemmatize:\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    if stem:\n",
    "        stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "    tokens = []\n",
    "    for current_text in list_of_texts:\n",
    "        current_tokens = tknzr.tokenize(current_text.lower())\n",
    "        if mwe_tuples_list is not None:\n",
    "            current_tokens = mwe_tknzr.tokenize(current_tokens)\n",
    "        if remove_punctuation:\n",
    "            current_tokens = [current_token for current_token in current_tokens if current_token not in string.punctuation]\n",
    "        if remove_stopwords:\n",
    "            current_tokens = [current_token for current_token in current_tokens if current_token not in stopwords]\n",
    "        if lemmatize:\n",
    "            current_tokens = [lemmatizer.lemmatize(current_token) for current_token in current_tokens]\n",
    "        if stem:\n",
    "            current_tokens = [stemmer.stem(current_token) for current_token in current_tokens]\n",
    "        tokens.append(current_tokens)\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block demonstrates the complete pipeline we have developed so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preparation took 12.54 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid', 303),\n",
       " ('food', 280),\n",
       " ('covid_-_19', 262),\n",
       " ('peopl', 133),\n",
       " ('u', 101),\n",
       " ('like', 92),\n",
       " ('death', 88),\n",
       " ('help', 76),\n",
       " ('go', 75),\n",
       " ('work', 72),\n",
       " ('time', 71),\n",
       " ('one', 70),\n",
       " ('need', 70),\n",
       " ('test', 64),\n",
       " ('day', 62),\n",
       " ('make', 61),\n",
       " ('think', 57),\n",
       " ('even', 55),\n",
       " ('life', 55),\n",
       " ('pandem', 54),\n",
       " ('still', 53),\n",
       " ('home', 53),\n",
       " ('know', 53),\n",
       " ('case', 51),\n",
       " ('die', 51),\n",
       " ('trump', 50),\n",
       " ('see', 49),\n",
       " ('would', 48),\n",
       " ('health', 47),\n",
       " ('viru', 47)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mwe_tuples = [\n",
    "    ('u', '.', 's', '.'),\n",
    "    ('covid', '-', '19'),\n",
    "]\n",
    "my_stopwords = ['..', '...', 'get', '\\u200d', '’', '“', '”']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# read raw data\n",
    "raw_data_filepath = pathlib.Path('relevant_data.json.gz')\n",
    "\n",
    "with gzip.GzipFile(raw_data_filepath, 'r') as fp:\n",
    "    json_bytes = fp.read()\n",
    "    json_str = json_bytes.decode('utf-8')\n",
    "    tweet_data = json.loads(json_str)\n",
    "    \n",
    "# convert to Pandas DataFrame\n",
    "tweet_df = pd.DataFrame(tweet_data, \n",
    "                        columns = column_names)\n",
    "\n",
    "tweet_df['Datetime'] = pd.to_datetime(tweet_df['Datetime'], \n",
    "                                      format = '%a %b %d %H:%M:%S %z %Y')\n",
    "tweet_df['Date'] = tweet_df['Datetime'].dt.date\n",
    "tweet_df['Day_Name'] = tweet_df['Datetime'].dt.day_name()\n",
    "\n",
    "weekend_mask = tweet_df['Day_Name'].isin(['Saturday', 'Sunday'])\n",
    "tweet_df.loc[weekend_mask, 'Weekend'] = True\n",
    "tweet_df.loc[~weekend_mask, 'Weekend'] = True\n",
    "\n",
    "tweet_df['Hour'] = tweet_df['Datetime'].dt.hour\n",
    "    \n",
    "# get sample\n",
    "np.random.seed(0)\n",
    "sample_size = 1000\n",
    "sample = np.random.choice(tweet_df['Text'].tolist(), \n",
    "                          size = sample_size,\n",
    "                          replace = False,\n",
    "                         )\n",
    "sample = sample.tolist()\n",
    "column_names = [\n",
    "    'Datetime',\n",
    "    'Username',\n",
    "    'User',\n",
    "    'Location',\n",
    "    'Language',\n",
    "    'Text',\n",
    "]\n",
    "\n",
    "# prepare tweets\n",
    "tweet_tokens = prepare_texts(sample, \n",
    "                             mwe_tuples_list = my_mwe_tuples,\n",
    "                             custom_stopword_list = my_stopwords,\n",
    "                            )\n",
    "    \n",
    "# get 30 most common tokens\n",
    "all_tokens = list(itertools.chain.from_iterable(tweet_tokens))\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time, 2)\n",
    "print(f'Text preparation took {elapsed_time} seconds')\n",
    "\n",
    "freq_dist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have only worked with individual tokens. However, in language, words used in close proximity are typically necessary for understaing. In NLP, a bigram refers to two consecutive tokens, a trigram refers to three consecutive tokens, and an $n$-gram refers to $n$ consecutive tokens. The following code block shows a simple way that `nltk` allows us to construct all bigrams for a specific set of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@olandgren', 'dementia'),\n",
       " ('dementia', 'mean'),\n",
       " ('mean', 'dad'),\n",
       " ('dad', 'listen'),\n",
       " ('listen', \"mother'\"),\n",
       " (\"mother'\", 'question'),\n",
       " ('question', 'day'),\n",
       " ('day', 'happen'),\n",
       " ('happen', 'still'),\n",
       " ('still', 'confus'),\n",
       " ('confus', 'covid'),\n",
       " ('covid', 'certainli'),\n",
       " ('certainli', 'understand'),\n",
       " ('understand', 'riot')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in nltk.ngrams(tweet_tokens[0], n = 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, using such an exhaustive approach would be difficult, not to mention computationally expensive. Moreover, many $n$-grams will be meaningless. Thus, we are most likely to be interested in frequently appearing $n$-grams. Thankfully, `nltk` contains some simpler methods for this purpose. The following code block shows how we can use `nltk` to identify the most common 50 bigrams in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', '19'),\n",
       " ('wear', 'mask'),\n",
       " ('test', 'posit'),\n",
       " ('🇦', '🇪'),\n",
       " ('😂', '😂'),\n",
       " ('🏽', '🍳'),\n",
       " ('👩', '🏽'),\n",
       " ('🙏', '🙏'),\n",
       " ('😭', '😭'),\n",
       " ('🍳', '👩'),\n",
       " ('save', 'life'),\n",
       " ('new', 'york'),\n",
       " ('social', 'distanc'),\n",
       " ('nurs', 'home'),\n",
       " ('covid_-_19', 'pandem'),\n",
       " ('🤣', '🤣'),\n",
       " ('❤', '️'),\n",
       " ('mortal', 'rate'),\n",
       " ('stay', 'home'),\n",
       " ('social', 'medium'),\n",
       " ('sign', 'petit'),\n",
       " ('conspiraci', 'theori'),\n",
       " ('immun', 'system'),\n",
       " ('due', 'covid'),\n",
       " ('make', 'sens'),\n",
       " ('hong', 'kong'),\n",
       " ('common', 'sens'),\n",
       " ('🤦', '♀'),\n",
       " ('death', 'toll'),\n",
       " ('food', 'bank'),\n",
       " ('cant', 'cant'),\n",
       " ('suprem', 'court'),\n",
       " ('pass', 'away'),\n",
       " ('covid', 'patient'),\n",
       " ('health', 'safeti'),\n",
       " ('open', 'letter'),\n",
       " ('look', 'forward'),\n",
       " ('broke', 'rule'),\n",
       " ('pleas', 'help'),\n",
       " ('last', 'night'),\n",
       " ('donald', 'trump'),\n",
       " ('via', '@chang'),\n",
       " ('via', '@gatewaypundit'),\n",
       " ('108', 'ambul'),\n",
       " ('@coopuk', '@aldiuk'),\n",
       " ('@lidlgb', '@coopuk'),\n",
       " ('@morrison', '@lidlgb'),\n",
       " ('georg', 'floyd'),\n",
       " ('lok', 'sabha'),\n",
       " ('pune', 'koregaon')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(tweet_tokens)\n",
    "finder.nbest(nltk.collocations.BigramAssocMeasures().likelihood_ratio, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the following code block shows how we can identify the most common 50 trigrams in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('due', 'covid', '19'),\n",
       " ('fight', 'covid', '19'),\n",
       " ('die', 'covid', '19'),\n",
       " ('post', 'covid', '19'),\n",
       " ('covid', '19', 'death'),\n",
       " ('diagnos', 'covid', '19'),\n",
       " ('@chrisbrainworm', 'covid', '19'),\n",
       " ('covid', '19', 'ccp'),\n",
       " ('covid', '19', 'https://t.co/fr4nnb0gf'),\n",
       " ('covid', '19', 'psa'),\n",
       " ('covid', '19', 'scenerio'),\n",
       " ('covid', '19', 't-shirt'),\n",
       " ('covid', '19', 'wreak'),\n",
       " ('covid', '19', 'yea'),\n",
       " ('fraud', 'covid', '19'),\n",
       " ('garbag', 'covid', '19'),\n",
       " ('jacki', 'covid', '19'),\n",
       " ('kick', 'covid', '19'),\n",
       " ('trueli', 'covid', '19'),\n",
       " ('nurs', 'covid', '19'),\n",
       " ('advanc', 'covid', '19'),\n",
       " ('code', 'covid', '19'),\n",
       " ('covid', '19', 'acceler'),\n",
       " ('covid', '19', 'creativ'),\n",
       " ('covid', '19', 'lay'),\n",
       " ('covid', '19', 'sanction'),\n",
       " ('contract', 'covid', '19'),\n",
       " ('covid', '19', 'pandem'),\n",
       " ('covid', '19', 'bet'),\n",
       " ('discharg', 'covid', '19'),\n",
       " ('covid', '19', 'extend'),\n",
       " ('non', 'covid', '19'),\n",
       " ('covid', '19', 'compar'),\n",
       " ('spread', 'covid', '19'),\n",
       " ('effect', 'covid', '19'),\n",
       " ('covid', '19', 'educ'),\n",
       " ('wife', 'covid', '19'),\n",
       " ('covid', '19', 'power'),\n",
       " ('covid', '19', 'flu'),\n",
       " ('covid', '19', 'treat'),\n",
       " ('wait', 'covid', '19'),\n",
       " ('covid', '19', '3'),\n",
       " ('covid', '19', 'social'),\n",
       " ('covid', '19', 'everi'),\n",
       " ('protect', 'covid', '19'),\n",
       " ('servic', 'covid', '19'),\n",
       " ('kill', 'covid', '19'),\n",
       " ('covid', '19', 'lockdown'),\n",
       " ('covid', '19', 'without'),\n",
       " ('right', 'covid', '19')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents(tweet_tokens)\n",
    "finder.nbest(nltk.collocations.TrigramAssocMeasures().likelihood_ratio, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of just keeping a fixed number of common $n$-grams, we can also specify a threshold. In particuar, the threshold will result in `nltk` only returning $n$-grams that appear in the corpus a number of times greater than or equal to the threshold. The follwoing code block shows how we can apply a threshold of 5 to bigrams and trigrams. I also create a *replacement test* for each bigram or trigram that we will use to replace tokens corresponding to the identified bigrams and trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 5\n",
    "\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_documents(tweet_tokens)\n",
    "trigram_finder.apply_freq_filter(threshold)\n",
    "trigrams = {trigram: '_'.join(trigram) for trigram in trigram_finder.ngram_fd}\n",
    "\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_documents(tweet_tokens)\n",
    "bigram_finder.apply_freq_filter(threshold)\n",
    "bigrams = {bigram: '_'.join(bigram) for bigram in bigram_finder.ngram_fd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified bigrams are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('covid', '19'): 'covid_19',\n",
       " ('covid_-_19', 'pandem'): 'covid_-_19_pandem',\n",
       " ('test', 'posit'): 'test_posit',\n",
       " ('due', 'covid'): 'due_covid',\n",
       " ('wear', 'mask'): 'wear_mask',\n",
       " ('covid', 'patient'): 'covid_patient',\n",
       " ('covid_-_19', 'case'): 'covid_-_19_case',\n",
       " ('😂', '😂'): '😂_😂',\n",
       " ('due', 'covid_-_19'): 'due_covid_-_19',\n",
       " ('mani', 'peopl'): 'mani_peopl',\n",
       " ('covid', 'death'): 'covid_death',\n",
       " ('put', 'food'): 'put_food',\n",
       " ('stay', 'home'): 'stay_home',\n",
       " ('save', 'life'): 'save_life',\n",
       " ('🙏', '🙏'): '🙏_🙏',\n",
       " ('nurs', 'home'): 'nurs_home',\n",
       " ('😭', '😭'): '😭_😭',\n",
       " ('covid_-_19', 'death'): 'covid_-_19_death',\n",
       " ('covid', 'case'): 'covid_case',\n",
       " ('covid', 'test'): 'covid_test',\n",
       " ('die', 'covid'): 'die_covid',\n",
       " ('food', 'bank'): 'food_bank',\n",
       " ('posit', 'covid_-_19'): 'posit_covid_-_19',\n",
       " ('pleas', 'help'): 'pleas_help',\n",
       " ('👩', '🏽'): '👩_🏽',\n",
       " ('🏽', '🍳'): '🏽_🍳',\n",
       " ('make', 'sens'): 'make_sens',\n",
       " ('social', 'medium'): 'social_medium',\n",
       " ('new', 'york'): 'new_york',\n",
       " ('posit', 'covid'): 'posit_covid',\n",
       " ('money', 'food'): 'money_food',\n",
       " ('🇦', '🇪'): '🇦_🇪',\n",
       " ('buy', 'food'): 'buy_food',\n",
       " ('social', 'distanc'): 'social_distanc',\n",
       " ('❤', '️'): '❤_️',\n",
       " ('🍳', '👩'): '🍳_👩',\n",
       " ('feel', 'like'): 'feel_like',\n",
       " ('go', 'back'): 'go_back',\n",
       " ('sign', 'petit'): 'sign_petit',\n",
       " ('deliv', 'food'): 'deliv_food',\n",
       " ('immun', 'system'): 'immun_system',\n",
       " ('#coronaviru', '#covid19'): '#coronaviru_#covid19',\n",
       " ('health', 'safeti'): 'health_safeti',\n",
       " ('peopl', 'die'): 'peopl_die',\n",
       " ('peopl', 'think'): 'peopl_think',\n",
       " ('post', 'covid'): 'post_covid',\n",
       " ('food', 'tabl'): 'food_tabl',\n",
       " ('covid_-_19', 'test'): 'covid_-_19_test',\n",
       " ('take', 'care'): 'take_care',\n",
       " ('fight', 'covid'): 'fight_covid',\n",
       " ('mortal', 'rate'): 'mortal_rate',\n",
       " ('🤣', '🤣'): '🤣_🤣',\n",
       " ('covid_-_19', 'patient'): 'covid_-_19_patient',\n",
       " ('death', 'toll'): 'death_toll',\n",
       " ('love', 'one'): 'love_one'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified trigrams are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('👩', '🏽', '🍳'): '👩_🏽_🍳',\n",
       " ('🏽', '🍳', '👩'): '🏽_🍳_👩',\n",
       " ('🍳', '👩', '🏽'): '🍳_👩_🏽',\n",
       " ('put', 'food', 'tabl'): 'put_food_tabl',\n",
       " ('test', 'posit', 'covid_-_19'): 'test_posit_covid_-_19',\n",
       " ('🙏', '🙏', '🙏'): '🙏_🙏_🙏'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines two functions that will allow us to replace identified bigrams and trigrams with the replacement text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_bigrams(token_list, bigrams_dict):\n",
    "    '''\n",
    "    This function accepts a list of tokens and a dictionary of possible bigrams.\n",
    "    An example of the expected format of the dictionary is:\n",
    "    \n",
    "    sample_bigram_dict = {\n",
    "        ('happy','birthday'): 'happy_birthday',\n",
    "        ('crash','course'): 'crash_course',\n",
    "    }\n",
    "    \n",
    "    The keys of the bigrams_dict specify the tokens that make\n",
    "    up a possible bigram. The values specify replacemnt text that is used to\n",
    "    replace the words in the key whenever they are encountered consecutivley.\n",
    "    Using our sample_bigram_dict, the function will replace consecutive tokens\n",
    "    'happy' and 'birthday' with 'happy_birthday'. Note that the replacement text\n",
    "    should have no spaces (it is recommended to join words with an underscore).\n",
    "    The function returns the list of tokens with any bigram tokens replaced.\n",
    "    \n",
    "    '''\n",
    "    new_tokens = []\n",
    "    items_to_skip = 0\n",
    "    for index in range(len(token_list)):\n",
    "        if index < len(token_list)-1:\n",
    "            if items_to_skip == 0:\n",
    "                current_tuple = (token_list[index], token_list[index+1])\n",
    "                if current_tuple in bigrams_dict:\n",
    "                    new_tokens.append(bigrams_dict[current_tuple])\n",
    "                    items_to_skip = 1\n",
    "                else:\n",
    "                    new_tokens.append(token_list[index])\n",
    "            else:\n",
    "                items_to_skip -=1\n",
    "        else:\n",
    "            if items_to_skip == 0:\n",
    "                new_tokens.append(token_list[index])\n",
    "                \n",
    "    return new_tokens\n",
    "\n",
    "def replace_trigrams(token_list, trigrams_dict):\n",
    "    '''\n",
    "    This function accepts a list of tokens and a dictionary of possible trigrams.\n",
    "    An example of the expected format of the dictionary is:\n",
    "    \n",
    "    sample_trigram_dict = {\n",
    "        ('one','more','time'): 'one_more_time',\n",
    "        ('all','is','well'): 'all_is_well',\n",
    "    }\n",
    "    \n",
    "    The keys of the trigrams_dict specify the tokens that make\n",
    "    up a possible trigrams. The values specify replacement text that is used to\n",
    "    replace the words in the key whenever they are encountered consecutivley.\n",
    "    Using our sample_trigram_dict, the function will replace consecutive tokens\n",
    "    'one', 'more', and 'time' with 'one_more_time'. Note that the replacement text\n",
    "    should have no spaces (it is recommended to join words with an underscore).\n",
    "    The function returns the list of tokens with any trigram tokens replaced.\n",
    "    \n",
    "    '''\n",
    "    new_tokens = []\n",
    "    items_to_skip = 0\n",
    "    for index in range(len(token_list)):\n",
    "        if index < len(token_list)-2:\n",
    "            if items_to_skip == 0:\n",
    "                current_tuple = (token_list[index], token_list[index+1], token_list[index+2])\n",
    "                if current_tuple in trigrams_dict:\n",
    "                    new_tokens.append(trigrams_dict[current_tuple])\n",
    "                    items_to_skip = 2\n",
    "                else:\n",
    "                    new_tokens.append(token_list[index])\n",
    "            else:\n",
    "                items_to_skip -=1\n",
    "        else:\n",
    "            if items_to_skip == 0:\n",
    "                new_tokens.append(token_list[index])\n",
    "            else:\n",
    "                items_to_skip -=1\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example of the `replace_bigrams` function, consider the fourth tweet in our sample. The tokens are printed by the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11,237', 'pple', 'discharg', 'covid', '19', 'treat', 'wld', 'safe', 'say', 'nt', 'report', 'case', 'trueli', 'covid', '19', 'bet', 'simpli', 'malaria', 'drug', 'give', 'b4', 'discharg', '9ja', 'dey', 'hail', 'una']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens after applying the `replace_bigrams` function are printed by the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['11,237', 'pple', 'discharg', 'covid_19', 'treat', 'wld', 'safe', 'say', 'nt', 'report', 'case', 'trueli', 'covid_19', 'bet', 'simpli', 'malaria', 'drug', 'give', 'b4', 'discharg', '9ja', 'dey', 'hail', 'una']\n"
     ]
    }
   ],
   "source": [
    "print(replace_bigrams(tweet_tokens[3], bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block demonstrates our final data preparation pipeline applied to a sample of 50,000 tweets with a bigram/trigram threshold of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Creating sample\n",
      "Tokenizing, normalizing, lemmatizing, and stemming\n",
      "Finding trigrams\n",
      "Finding bigrams\n",
      "Replacing bigrams and trigrams\n",
      "Text preparation took 65.24 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(':/', 1754),\n",
       " ('covid_-_19', 1740),\n",
       " ('covid', 1408),\n",
       " ('like', 1236),\n",
       " ('food', 1235),\n",
       " ('u', 1059),\n",
       " ('#covid19', 1008),\n",
       " ('go', 973),\n",
       " ('peopl', 970),\n",
       " ('one', 961),\n",
       " ('work', 954),\n",
       " ('need', 941),\n",
       " ('use', 910),\n",
       " ('time', 894),\n",
       " ('say', 889),\n",
       " ('…', 886),\n",
       " ('death', 881),\n",
       " ('pandem', 865),\n",
       " ('see', 861),\n",
       " ('know', 854),\n",
       " ('covid_19', 847),\n",
       " ('think', 835),\n",
       " ('thank', 832),\n",
       " ('case', 824),\n",
       " ('help', 820),\n",
       " ('today', 813),\n",
       " ('day', 807),\n",
       " ('also', 797),\n",
       " ('even', 790),\n",
       " ('right', 786),\n",
       " ('19', 747),\n",
       " ('make', 743),\n",
       " ('test', 731),\n",
       " ('call', 729),\n",
       " ('come', 709),\n",
       " ('2', 702),\n",
       " ('look', 693),\n",
       " ('@realdonaldtrump', 692),\n",
       " ('support', 690),\n",
       " ('well', 682),\n",
       " ('report', 670),\n",
       " ('trump', 652),\n",
       " ('state', 649),\n",
       " ('countri', 648),\n",
       " ('still', 641),\n",
       " ('home', 634),\n",
       " ('viru', 627),\n",
       " ('via', 624),\n",
       " ('1', 620),\n",
       " ('want', 616)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mwe_tuples = [\n",
    "    ('u', '.', 's', '.'),\n",
    "    ('covid', '-', '19'),\n",
    "]\n",
    "my_stopwords = ['..', '...', 'get', '\\u200d', '’', '“', '”']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# read raw data\n",
    "print('Loading data')\n",
    "raw_data_filepath = pathlib.Path('relevant_data.json.gz')\n",
    "\n",
    "with gzip.GzipFile(raw_data_filepath, 'r') as fp:\n",
    "    json_bytes = fp.read()\n",
    "    json_str = json_bytes.decode('utf-8')\n",
    "    tweet_data = json.loads(json_str)\n",
    "    \n",
    "# convert to Pandas DataFrame\n",
    "tweet_df = pd.DataFrame(tweet_data, \n",
    "                        columns = column_names)\n",
    "\n",
    "tweet_df['Datetime'] = pd.to_datetime(tweet_df['Datetime'], \n",
    "                                      format = '%a %b %d %H:%M:%S %z %Y')\n",
    "tweet_df['Date'] = tweet_df['Datetime'].dt.date\n",
    "tweet_df['Day_Name'] = tweet_df['Datetime'].dt.day_name()\n",
    "\n",
    "weekend_mask = tweet_df['Day_Name'].isin(['Saturday', 'Sunday'])\n",
    "tweet_df.loc[weekend_mask, 'Weekend'] = True\n",
    "tweet_df.loc[~weekend_mask, 'Weekend'] = True\n",
    "\n",
    "tweet_df['Hour'] = tweet_df['Datetime'].dt.hour\n",
    "    \n",
    "# get sample\n",
    "print('Creating sample')\n",
    "np.random.seed(0)\n",
    "sample_size = 50000\n",
    "sample = np.random.choice(tweet_df['Text'].tolist(), \n",
    "                          size = sample_size,\n",
    "                          replace = False,\n",
    "                         )\n",
    "sample = sample.tolist()\n",
    "column_names = [\n",
    "    'Datetime',\n",
    "    'Username',\n",
    "    'User',\n",
    "    'Location',\n",
    "    'Language',\n",
    "    'Text',\n",
    "]\n",
    "\n",
    "# prepare tweets\n",
    "print('Tokenizing, normalizing, lemmatizing, and stemming')\n",
    "tweet_tokens = prepare_texts(sample, \n",
    "                             mwe_tuples_list = my_mwe_tuples,\n",
    "                             custom_stopword_list = my_stopwords,\n",
    "                            )\n",
    "    \n",
    "threshold = 3\n",
    "\n",
    "print('Finding trigrams')\n",
    "trigram_finder = nltk.collocations.TrigramCollocationFinder.from_documents(tweet_tokens)\n",
    "trigram_finder.apply_freq_filter(threshold)\n",
    "trigrams = {trigram: '_'.join(trigram) for trigram in trigram_finder.ngram_fd}\n",
    "\n",
    "print('Finding bigrams')\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_documents(tweet_tokens)\n",
    "bigram_finder.apply_freq_filter(threshold)\n",
    "bigrams = {bigram: '_'.join(bigram) for bigram in bigram_finder.ngram_fd}\n",
    "\n",
    "print('Replacing bigrams and trigrams')\n",
    "for current_index, current_tweet in enumerate(tweet_tokens):\n",
    "    tweet_tokens[current_index] = replace_trigrams(current_tweet, trigrams)\n",
    "    tweet_tokens[current_index] = replace_bigrams(current_tweet, bigrams)\n",
    "\n",
    "all_tokens = list(itertools.chain.from_iterable(tweet_tokens))\n",
    "freq_dist = nltk.FreqDist(all_tokens)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = round(end_time - start_time, 2)\n",
    "print(f'Text preparation took {elapsed_time} seconds')\n",
    "\n",
    "freq_dist.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block uses `matplotlib` to plot the frequence of the top 50 most common tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBsAAAGkCAYAAABw5i3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZwdVZn4/88DQUAWAQkQku4EJaCAyhIQEQUHlVUWBQUFWQ0gi4q4oOO44iDihiiCgqCoiCCCGyPj7rhGZVQUZkAk3SQKM/7mq9/vzDgDnt8fz7l2pe1036WSdMLn/Xr1q2/VrTr31K2qU+c851TdKKUgSZIkSZLUlrVWdQYkSZIkSdKaxWCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVs1Y1RmYyuabb17mzZu3qrMhSZIkSZIafvzjH/9bKWXmRO9N+2DDvHnzWLRo0arOhiRJkiRJaoiIe5f3nrdRSJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa2aMtgQEVdGxP0R8YvGvE9HxG317zcRcVudPy8i/qvx3oca6+wWET+PiLsi4uKIiBWzSZIkSZIkaVWa0cUyVwGXAB/rzCilvKDzOiLeBfyfxvJ3l1J2niCdS4GFwPeBLwEHAF/uPcuSJEmSJGk6m3JkQynlW8DvJ3qvjk54PvCpydKIiFnAxqWU75VSChm4OLz37EqSJEmSpOlu0Gc2PA34XSnlXxvztomIn0bENyPiaXXebGC0scxonde1eUNDRETff/OGhgbbUkmSJEmS1JVubqOYzDEsO6phKTBcSvn3iNgN+FxE7AhM9HyGsrxEI2IhecsFw8PDANw7Osr9H/x43xnd4qXH9b2uJEmSJEnqXt8jGyJiBvBc4NOdeaWUP5VS/r2+/jFwN7AdOZJhTmP1OcCS5aVdSrm8lLKglLJg5syZ/WZRkiRJkiStAoPcRvFM4I5Syl9uj4iImRGxdn39GGA+8OtSylLgjxGxZ33Ow4uBmwb47IF5W4YkSZIkSSvGlLdRRMSngH2BzSNiFHhjKeUK4Gj++sGQTwfeEhEPAg8Bp5VSOg+XPJ38ZYv1yV+hWKW/RHHv6Cj3X3pZ3+tvcfqpLeZGkiRJkqQ1x5TBhlLKMcuZf8IE824AbljO8ouAnXrMnyRJkiRJWs0M+msUkiRJkiRJyzDYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWrVlMGGiLgyIu6PiF805r0pIu6LiNvq30GN986LiLsi4s6I2L8xf7eI+Hl97+KIiPY3R5IkSZIkrWrdjGy4CjhggvnvKaXsXP++BBAROwBHAzvWdT4YEWvX5S8FFgLz699EaUqSJEmSpNXclMGGUsq3gN93md5hwLWllD+VUu4B7gL2iIhZwMallO+VUgrwMeDwfjMtSZIkSZKmr0Ge2XBmRPys3maxaZ03GxhpLDNa582ur8fPn1BELIyIRRGx6IEHHhggi5IkSZIkaWXrN9hwKfBYYGdgKfCuOn+i5zCUSeZPqJRyeSllQSllwcyZM/vMoiRJkiRJWhX6CjaUUn5XSnmolPJn4MPAHvWtUWCosegcYEmdP2eC+ZIkSZIkaQ3TV7ChPoOh4wig80sVNwNHR8S6EbEN+SDIH5ZSlgJ/jIg9669QvBi4aYB8S5IkSZKkaWrGVAtExKeAfYHNI2IUeCOwb0TsTN4K8RvgVIBSyu0RcR3wS+BB4IxSykM1qdPJX7ZYH/hy/ZMkSZIkSWuYKYMNpZRjJph9xSTLnw+cP8H8RcBOPeVOkiRJkiStdgb5NQpJkiRJkqS/YrBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpgQ0vmDQ0REX3/zRsaWtWbIEmSJElSK2as6gysKe4dHeW3H7yo7/W3eum5LeZGkiRJkqRVx5ENkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElq1ZTBhoi4MiLuj4hfNOa9MyLuiIifRcSNEbFJnT8vIv4rIm6rfx9qrLNbRPw8Iu6KiIsjIlbMJkmSJEmSpFWpm5ENVwEHjJt3K7BTKeWJwL8A5zXeu7uUsnP9O60x/1JgITC//o1PU5IkSZIkrQGmDDaUUr4F/H7cvK+UUh6sk98H5kyWRkTMAjYupXyvlFKAjwGH95dlSZIkSZI0nbXxzIaTgC83preJiJ9GxDcj4ml13mxgtLHMaJ03oYhYGBGLImLRAw880EIWJUmSJEnSyjJQsCEiXg88CHyizloKDJdSdgHOAT4ZERsDEz2foSwv3VLK5aWUBaWUBTNnzhwki5IkSZIkaSWb0e+KEXE8cAiwX701glLKn4A/1dc/joi7ge3IkQzNWy3mAEv6/WxJkiRJkjR99TWyISIOAF4DHFpK+c/G/JkRsXZ9/RjyQZC/LqUsBf4YEXvWX6F4MXDTwLlfg80dmkNE9P03d2jOuPRmD5jecu96kSRJkiRpGVOObIiITwH7AptHxCjwRvLXJ9YFbq2/YPn9+ssTTwfeEhEPAg8Bp5VSOg+XPJ38ZYv1yWc8NJ/zoHEWj97Hkg+cN/WCy7H1GX8/Lr0lLL74hX2nN3z2J/teV5IkSZL08DJlsKGUcswEs69YzrI3ADcs571FwE495U6SJEmSJK122vg1CkmSJEmSpL8w2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQb1Ze7QbCKi77+5Q7NX9SZIkiRJklaQGas6A1o9LR5dwq8uObTv9R9/5s0t5kaSJEmSNJ04skGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrpgw2RMSVEXF/RPyiMW+ziLg1Iv61/t+08d55EXFXRNwZEfs35u8WET+v710cEdH+5kiSJEmSpFWtm5ENVwEHjJv3WuCrpZT5wFfrNBGxA3A0sGNd54MRsXZd51JgITC//o1PU5IkSZIkrQGmDDaUUr4F/H7c7MOAq+vrq4HDG/OvLaX8qZRyD3AXsEdEzAI2LqV8r5RSgI811pGYOzSbiOj7b+7Q7FW9CZIkSZKkakaf621ZSlkKUEpZGhFb1Pmzge83lhut8/63vh4/XwJg8egSfnzpIX2vv9vpX2gxN5IkSZKkQbT9gMiJnsNQJpk/cSIRCyNiUUQseuCBB1rLnCRJkiRJWvH6DTb8rt4aQf1/f50/Cgw1lpsDLKnz50wwf0KllMtLKQtKKQtmzpzZZxYlSZIkSdKq0G+w4Wbg+Pr6eOCmxvyjI2LdiNiGfBDkD+stF3+MiD3rr1C8uLGOJEmSJElag0z5zIaI+BSwL7B5RIwCbwQuAK6LiJOBxcBRAKWU2yPiOuCXwIPAGaWUh2pSp5O/bLE+8OX6J0mSJEmS1jBTBhtKKccs5639lrP8+cD5E8xfBOzUU+6kPg0PzWZkdLl36kxpaM7WLB65r8UcSZIkSdLDR7+/RiFNayOjS/jOZQf3vf7ep35xmWmDF5IkSZLUPYMNUhdGRpdw64cP7Hv9Z73Eu4YkSZIkPXy0/dOXkro0PDSbiOj7b3ho9qreBEmSJEmakCMbpFVkZHQJn7/igL7Xf87JtywzPTy0NSOjS/tOb2jOLBaP9H+riCRJkiR1GGyQ1hAjo0v5zJX7973+USf9Q4u5kSRJkvRw5m0UkiY0NLT1QLd5DA1tvao3QZIkSdIq4sgGSRMaHV3Kxz/67L7XP+7Er7SYG0mSJEmrE0c2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpgg6SVwl+3kCRJkh4+/DUKSSvF6OhSPnJ1/79uccrx/rqFJEmStLpwZIOk1ZIjJSRJkqTpy5ENklZLo6NLueTj/Y+UOPO4ZUdKDA1tzejo0r7TmzNnFiMjS/peX5IkSVqTGGyQJDJ48c5P9h+8eNULlw1ezBnemvtG+g9ezB6axehigxeSJElaPRlskKQV4L6Rpbzx0/0HL978AoMXkiRJWn0ZbJCk1cB9I0t52fX7973++478hxZzI0mSJE3OB0RKkiRJkqRWGWyQpIeh2cOD/ZrH7OGtx6U3e8D0Zq+ib0KSJEkrgrdRSNLD0JKRpRz1uf5vy/jM4cvelrFkZAkHfO6kvtO75fArl5mePTyHJSP39Z3e1kOzuW/xaN/rS5IkaTAGGyRJ086Skfs48MZX973+l4+4sMXcSJIkqVfeRiFJWuPNHp4z0G0eeavHnFW9GZIkSasNRzZIktZ4S0bu48DPvnWgNL783De0lBtJkqQ1nyMbJEnqw+zhoQFHSgyt0PQkSZJWJUc2SJLUhyUjoxx0w0V9r/+l5507QXrvHyC9s/peV5IkqW2ObJAkaQ3kyAtJkrQqObJBkqQ10JKRUQ66/rK+1//Skaf+VXoHX//RvtP74pEn9r2uJEla/TiyQZIkrXSOlJAkac3myAZJkrTSLRkZ5eDPXNP3+l886thlpmcPD7NkZKTv9LYeGuK+xYv7Xl+SJC3LYIMkSVrtLRkZ4eDPfLrv9b941AtazI0kSfI2CkmSpHFmDw8PeJvH8KreBEmSVilHNkiSJI2zZGSEQz5zY9/rf+GoI5aZ9jYPSdLDjcEGSZKkFWzJyAiHfuaLfa9/81EHLzNt8EKSNN0ZbJAkSVrNLBkZ4bDP3Nr3+jcd9awWcyNJ0l/zmQ2SJEmSJKlVfQcbImL7iLit8feHiHh5RLwpIu5rzD+osc55EXFXRNwZEfu3swmSJEmSJGk66fs2ilLKncDOABGxNnAfcCNwIvCeUspFzeUjYgfgaGBHYGvgHyNiu1LKQ/3mQZIkSZIkTT9t3UaxH3B3KeXeSZY5DLi2lPKnUso9wF3AHi19viRJkiRJmibaCjYcDXyqMX1mRPwsIq6MiE3rvNlA87HJo3XeX4mIhRGxKCIWPfDAAy1lUZIkSROZPTxMRPT9N3t4eFVvgiRpmhn41ygi4hHAocB5ddalwFuBUv+/CzgJiAlWLxOlWUq5HLgcYMGCBRMuI0mSpHYsGRnh8Ou/3ff6nzvyactMzx6ey5KRwX5ac+uhYe5bPNmgWUnSdNbGT18eCPyklPI7gM5/gIj4MPCFOjkKDDXWmwMsaeHzJUmSNI0sGVnMc6//0UBpfPbI3VvKjSRpVWjjNopjaNxCERGzGu8dAfyivr4ZODoi1o2IbYD5wA9b+HxJkiSt4WYPzx3wVo+5q3oTJOlhZaCRDRHxSOBZwKmN2RdGxM7kLRK/6bxXSrk9Iq4Dfgk8CJzhL1FIkiSpG0tGFnPU9bf3vf5njtxxmelBb/XwNg9JmtxAwYZSyn8Cjx4377hJlj8fOH+Qz5QkSZIGtWRkMSfc0H+w4KrnOVJCkibT1q9RSJIkSZIkAQYbJEmSpIHNGfCZEnN8poSkNUwbv0YhSZIkPazdN7KYN372vr7Xf/NzZy8zPTQ8j9GR/m/zmDM0l5HFv+l7fUkalMEGSZIkaZoZHbmXSz77277XP/O5W7WYG0nqnbdRSJIkSWu4oeF5A93mMTQ8b5n0hgdMb3hcepLWPI5skCRJktZwoyP3cs319/e9/rFHbrHM9MjIvdx03QN9p3fY82cuMz08NI+R0f5vGxmaM5fFI7/pe31J7TPYIEmSJGmVGhm9l1s/0X8w5FkvWjYYYvBCWvUMNkiSJElao4yM3ss/Xd1/8OKpx28x9UKSJuUzGyRJkiRpEnOHBntGxdyheat6E6SVzpENkiRJkjSJxaP38pMP/67v9Xd9yZYt5kZaPTiyQZIkSZIktcpggyRJkiStRN6WoYcDb6OQJEmSpJVo8ei93HHJb/te/3FnbrXM9NyhuSweXdx3esNzhrl3ZOzXOwZNb6I09fBjsEGSJEmSVmOLRxez+KKlfa8/fO6sv0pv6YX3DJSnWa/eZpnptgMimv4MNkiSJEmSVqjFo4tZ+q7b+15/1it3XGZ63tBc7h0geDF3zjC/MXixQhlskCRJkiStVu4dXcxv37Oo7/W3esWCFnOjifiASEmSJEnSw9q8oeGBHto5b2h4haa3OnJkgyRJkiTpYe3e0RF++97v9L3+Vi/f+6/S+93FX+07vS3P3m+Z6XlDw9w7OtJ3enPnDPGbkcEe+tkrgw2SJEmSJE1j946O8Lv3f7Hv9bc86+BlpldG8MJggyRJkiRJDyP3jo7wu0tu6Hv9Lc983pTL+MwGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLVqoGBDRPwmIn4eEbdFxKI6b7OIuDUi/rX+37Sx/HkRcVdE3BkR+w+aeUmSJEmSNP20MbLhGaWUnUspC+r0a4GvllLmA1+t00TEDsDRwI7AAcAHI2LtFj5fkiRJkiRNIyviNorDgKvr66uBwxvzry2l/KmUcg9wF7DHCvh8SZIkSZK0Cg0abCjAVyLixxGxsM7bspSyFKD+36LOnw2MNNYdrfP+SkQsjIhFEbHogQceGDCLkiRJkiRpZZox4PpPLaUsiYgtgFsj4o5Jlo0J5pWJFiylXA5cDrBgwYIJl5EkSZIkSdPTQCMbSilL6v/7gRvJ2yJ+FxGzAOr/++vio8BQY/U5wJJBPl+SJEmSJE0/fQcbImKDiNio8xp4NvAL4Gbg+LrY8cBN9fXNwNERsW5EbAPMB37Y7+dLkiRJkqTpaZDbKLYEboyITjqfLKXcEhE/Aq6LiJOBxcBRAKWU2yPiOuCXwIPAGaWUhwbKvSRJkiRJmnb6DjaUUn4NPGmC+f8O7Lecdc4Hzu/3MyVJkiRJ0vS3In76UpIkSZIkPYwZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJa1XewISKGIuLrEfGriLg9Il5W578pIu6LiNvq30GNdc6LiLsi4s6I2L+NDZAkSZIkSdPLjAHWfRB4ZSnlJxGxEfDjiLi1vveeUspFzYUjYgfgaGBHYGvgHyNiu1LKQwPkQZIkSZIkTTN9j2wopSwtpfykvv4j8Ctg9iSrHAZcW0r5UynlHuAuYI9+P1+SJEmSJE1PrTyzISLmAbsAP6izzoyIn0XElRGxaZ03GxhprDbKcoITEbEwIhZFxKIHHnigjSxKkiRJkqSVZOBgQ0RsCNwAvLyU8gfgUuCxwM7AUuBdnUUnWL1MlGYp5fJSyoJSyoKZM2cOmkVJkiRJkrQSDRRsiIh1yEDDJ0opnwUopfyulPJQKeXPwIcZu1ViFBhqrD4HWDLI50uSJEmSpOlnkF+jCOAK4FellHc35s9qLHYE8Iv6+mbg6IhYNyK2AeYDP+z38yVJkiRJ0vQ0yK9RPBU4Dvh5RNxW570OOCYidiZvkfgNcCpAKeX2iLgO+CX5SxZn+EsUkiRJkiStefoONpRSvsPEz2H40iTrnA+c3+9nSpIkSZKk6a+VX6OQJEmSJEnqMNggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktcpggyRJkiRJapXBBkmSJEmS1CqDDZIkSZIkqVUGGyRJkiRJUqsMNkiSJEmSpFYZbJAkSZIkSa0y2CBJkiRJklplsEGSJEmSJLXKYIMkSZIkSWqVwQZJkiRJktQqgw2SJEmSJKlVBhskSZIkSVKrDDZIkiRJkqRWGWyQJEmSJEmtMtggSZIkSZJaZbBBkiRJkiS1ymCDJEmSJElqlcEGSZIkSZLUKoMNkiRJkiSpVQYbJEmSJElSqww2SJIkSZKkVhlskCRJkiRJrTLYIEmSJEmSWmWwQZIkSZIktWqlBxsi4oCIuDMi7oqI167sz5ckSZIkSSvWSg02RMTawAeAA4EdgGMiYoeVmQdJkiRJkrRireyRDXsAd5VSfl1K+R/gWuCwlZwHSZIkSZK0Aq3sYMNsYKQxPVrnSZIkSZKkNUSUUlbeh0UcBexfSjmlTh8H7FFKOWvccguBhXVye+DOLpLfHPi3FrNreqa3KtNbEWmanumZ3spN0/RMz/RWXnorIk3TMz3TW7lpmt7qmd7cUsrMid6Y0WJmujEKDDWm5wBLxi9USrkcuLyXhCNiUSllwWDZMz3Tmx7prYg0Tc/0TG/lpml6pmd6Ky+9FZGm6Zme6a3cNE1vzUtvZd9G8SNgfkRsExGPAI4Gbl7JeZAkSZIkSSvQSh3ZUEp5MCLOBP4BWBu4spRy+8rMgyRJkiRJWrFW9m0UlFK+BHxpBSTd020Xpmd60zy9FZGm6Zme6a3cNE3P9Exv5aW3ItI0PdMzvZWbpumtYemt1AdESpIkSZKkNd/KfmaDJEmSJElawxlskCRJkiRJrVotgg0R8aRVnYduRMS0/T4jIlZ1HpZnOueto/56ysPK6rBfpF54TE9PK+La6b7WZCJiOCI2X9X50LKa5+2g5UJEbDA+TfVmBZXN07atpBVj2u/wiNgVeEFEbDAdC4yI2CEiLo2IGaWUPw+ax7a3MSJ2j4hNSyllOn5/1TIN+emWz4jYDHhbPRannXEX57VbSG/viNijTNMHuqzo42O6XQgn2t6287gCyp1Wy8EWKp3rRkS0dUyvqGOwjfN3dRIRTwAopfy5xTR3qGn2va9XxjVokM+IiPUiYtP6elbbwfA2tn8llNODfH+PBl4LHF9fT0sRsW1ELIiIdVtKb9oe1xGxVkRsXuuqu9byuq9yIdI2wKKIeEKb9d/pdq1cXpotncN7As9pIUud9PaKiF1rW2ngOkxnG1fg9XhatUU62qonRMSMiBiur7ePiA3bSHci06pSvRy/B+YAu7ZRYETE+m1V1Gs6AawLXBQRaw+Sx2ZlOCKeGBGb1IbuIF4K3BoRm7QdcKiN0oUDXvSfDXwiIs6NiNMhK4kt7OetawNjwzo9yD7fAvhf4ISIeOIg+WqKiJkRscmg6TSOmaOAgwbOGOwK3NQJrqyoi2G/6zW2d62IWKfzuoU8PS0idm6z4TOoiFivsb27RcTukI2zfre5cYGeHxELanoDN8IjYt+I2CUi5g96Dje2+aCIWGeQQG5E7AhcCnwoIvaPiPn95qumt1bznIuIvxkgrc0ar58PvKPlMnrniHjsoJWIifI06DlX1z8/Ik4ZJJ1xaQZwdUS8dJA0Gvt3l4gYiojZLeVvl4jYLCIeWc+Rnr/Dus7O5PXoVOAC4FED5muPiHhqszwY8Lre/A6fXK/HQwOk94iIeGx9/dyI2LjfMisi5pL1yi8Cw8ALo8URDm1sb03nEOCzwDuBqyJiu1YymGk/tcV68B617N8LBjp2dgSujIizgI8BOwyQrfVLKfcA1wIfj4gd2jima1n/yIg4NCLOHCB/zTJ1RbTD1mrjmg48CPyoloEbtJDeU4DrOvWsQY7BZhkDrN9C3pppD9d6R6sB6xbaNdsBlFIeigEDDjUvewHPjYgPAO8l9/cKMe2DDaWU3wCfBN7eqcT2m1Zkj8ctwPsi4v2dhkqfaa1VSvlzKeV28qL1OLLi1LtMUsMAACAASURBVHfAoXFxPgt4H/Aa4D39XAg7B2Ip5URgBLgmWhrh0CggHgM8ETi2nzQjYm/gYnL/fgc4OSLeVfM9SNDmAOAG4EPkxWvbQQq2UsodNY/3A6dGCwGHiDgbuAq4JCL+vs809o6I1zRmHQT8vwHytBZAKeVi4FPAx6KOcGijARQRh0XEMRGxe7/nceMcOZvcvx+LiGe0FCDYFTgpMoixysvGyF7fEyNiw9p4uha4ICK+BP0HHOr+PJixsvALLTQczwDeDvwN8O2IGO5nH3eOs8bx9nzgVeMqFr2ktyXwaXJbvwk8E3hlROzUa1o1vZ2Aw6P2KpPn3P19pjUMXBgRB9ZZM4C7Wxx98SqyoXIBef18TL9p1WPmgIh4XUS8Nfocyddcvp6zNzJ4Q7mZZgHOB/oOMjfKmJcDHwDeT35/hw2Yz5cCNwPvAt4QETP7OYfr97YE2Ad4C3BrKeWBfs/hWhZ8GDgYODsiLquf03e53/gOXwZcCJxBljVz+0kP2JZsNH6IHJHwyH4SqXWp19Q0Oj/Dvj1wTD/1rAnSb2V7a8P9IuD4UsozgP+v5rnffO0cEY+r+3QW8B4GaKA1yuk9gU8AJ5DH9F/qb72mWUr5OfCvZKPn3aWU2/tpUEXE9mR9d4dSypvJ6+Z1MWDAoW7T7sDXgHcAv+wnnWZ6kZ1tV0bEK+p5OLCIeAlwWUS8ttaF+0kjah4XAesBN5FBub6OmciOnA1LKe8iy9OrImKXAevlnTJmIbm950YG6AYS2cn2d8D+/bYRm/WVyODjLlE7jvo5pms6hwC3RcQnYfCAQ83fnWQd5ijgqlLKf3fy32+6y7PKK9TdKKXcAlwDPBX6q0BExFbAR4DLgFcAjwX+MSK27TNPf67pnkMWtPeSvQ0X14pYv70W+wLPA/YHtiYroP/e60FVSnmopncc8Dvg8cDXo52Aw2Pr/2uAbwO7AC/uI83NgatLKZ8tpXwfuBpYGBEX1W3op3HxGDKA8Wrg74EfkiMnhnptkI6rxN4OXEELAYeIOBo4DDiWDA48oc+kHgDOiohOJeRRQOe4bN5a0dV+aRzTpwEbAEuBWyLiyS30CpxOVvI2BL4XEfsMkNZLgEOBN5OjTk7oN61xfgzMhL805Ff1ELrZZOP4VLLs272Ush+wQT8Bh0YFcW1gG+B5pZSnkteBayNiRj+ZjHymzqE1r+sA/wyMDtLQI3tAAK4kb7Maam5DD7YB7iqlXFdK+SRwG7ATcEqfZf/eZABk34h4JHm+rdfJW4/bHMAdZPDi6XVeW4GGvYC/KaU8i2yozALu6XUfN46ZnckK9r+RAeZbo/Yu97JP6vK7Nxph3wJOiwFGh9Q0nxJjAaCfk43H3fsNQkZ2TBwHHAC8nAyKnBgRu/WZ3qHk+bw3eR0pZONs817KmhgLCC8GfkU2fHaMiMc3yu+u90c9hs8EzimlvA74W+CIiLiyfk5Px+O4686TgUNLKfuQ5er/AIv7KWdKKb8EbiXL+g+UUn4bY6PaeikT/g8ZeJxF1gO/AnyesYBD37dURMQetLS91QWllJ/W128ENos+bqeIvMXmIDL4sR3wW3Kk5n8P0tCLiP2A44EXlVJOAE4BdoqIc/tJMyKeCfyJrLe9PCJ269Rje8zbnWQw7u8iAywXkHXVvgMOje/pU2SdbQnZ8JtR3++ns21X4FXAIrJedEoMOMorcqTTC8ky5kAyINlrGs2G8kvI3u/XAS8gz5F+Ag7HA/8SGXB4Hzly5aMxYMAhIk4gy+mLgGOAPfpJZ5zbgcVkfeZZvZQznWXGdRxfXPP4jcgRbT0HCSJHlZxJXov+JyKuqZ/TT1rNds3vgMvJgOHj6zndOb9bvTVv2gcbIuIFtQF5N7Ag6oiCPg7OdYDfAF8opTwInExe/F/e48V5TozdK/ko4BAy+nwqWXCsT97fP6PPxu2DZBTxRGAr4KR64O7V64UmIp4GvJ6M0s0Hfgp8JQYIOET2xt0aEcfV7buhpvsisjI2aZrj3l+XvGeyU0ncCHglsEtE7N9H3maRjeR/LKV8m2xkXAT8gOxx7Un9jg6JiMsj4gqyN+UK8mJ9SkTs0mua1X+RDe8XA/OAI2r+u06vngd3kheAEyPiRLKSPScysv/4iHhCRDyqlwpjZM/t2cDbakPlb4EbI2LPPoM/a0XeO7kfefF7iOxh/k5jmV6Pww3Iwvv5ZOXk5Mhhtlv0kb89IwOGlFK+A/w32ahq5daCftQGQCfI+nFgN2BLsoJMrcyuHxH/VKe7Kmfq8XwwWbk+gqxgU0o5iNp70WfF+A9k0PFsYF/gsJqno6OP28Ai4vHAlyPibWRjficymNHPPvkX8rs6rU7PIBv4f6IGMLrMU6cS8SHgq2SwsBMgGK7/H9F43U1a9wLXkT1kzyd7F/aNvF3m2ZEjgXo+pht+EhFvJLfz2E7PXDfXzsih/nPrMbMHWcm5tJRyeSnlMPJ6fHPdjl73yXOAmyOHIT+abPQtqJ/bb8XpWOD6iHgzWXF/JxkAn9FLJXGcP5RS/lBydOW3gfvoYv+OS3eten27FnhK3effI4MX/5cc2bL5VN9hjAUZ/hwRO9Uy9U3A6WTg4vSI2LRen5/ZQ5kawB/JDolOEONq4MlRe6l70ahkr0uWpd+LHIU2BJxQ398nerytJ3KU1/eB04DXR8QhpZT/rW9PWWY1zrn/Bb5c/+azbMBhW3JkW09lVuO7XouWtpess3y2pr82WVeaC2xc53UdFCml/A+5T79GNuQXAN+oDfnO/uqnB3cfMhC+ef2c+8he61m9JhTZ8D4AuLmU8rc1v9dE3gK2d0S8s4s0onGenEmer2+PZQMOn4yIJ/ZYJ4p63u1Jjko6mgxafwDo3NqyVfdbC5G38n0auKGU8n6yQfoB4On1HO7XTOBI8paU/yYDmjMiO1u70jiHjyQ7Fv+p1kXOJ+tdL+jUUaZS9yullFOAz5HXpA1LKe9mLODQ1y0V9ZjdElhIBsB/T4706rQFukmjGRw9KCKeU0r5Ezka8Ldk++7ptT3XzTEzp5HeoeSx8gwySD8fuL1u/0O9bG8p5f8BJ5EjrM8F1otGwKHbdOrynf373MiRNbeTnbMBHBA5CuMw4OA+6uaTfvC0/iMr2o+urz8IXN5HGmuRBfV1ZKV1K7JxfAl5oXlXl+lsAbyNrMw8gqyg/wJ4Vn3/EcA5ZOXxgi7S27Tx+hiyp3YeOZTstsZ7p5IR1Q2mSC/GTW9PDtVftzHvu/Xg2mSAffIc4CfAMY15XyIL4kdNse7G46YvIYMVnyAvgJAN3Of3mKcnkKNWXk1eZE5svPcWsuem1+3cAfgnshFwPtmTtBN5cX07OYx/oz7SPRb4DzLw1Zl3Sv0u1u92H5MBs3OBJ5G9yf9DRik/Qxbs1wFb9XjMbEZelGcBM+q8y8kL1xP7PF7WIiPjHyZ7ldap818JzOslf3XeO8lG5Kca8zojJ9buMW971n38PjIw9zQySr5ev+fHJJ81E3j/FMtsSFa69qzn/eFkoOy6eozMbSz7JWCoh8+fD1xf03lX3a/PbLz/D+SzcXrZnnXIcu/7wNJxx/g/AjN7/I42JRsPnyDL5oPq6/vJXsNu0tgIeER9vQ4ZWPka2Tj+Qc33S4BPdpneRMfgSWRQ+BfAzxirzH8V2LybtGo+o/4/px6Hv67n9LX1HH5Mj9/fAfX7m1fz9y1gw/reS4FvMK4MniCN9YG31r/ZZG/RP9fjZbPGcjcCs7r9/prLksORXwz8qB47vwIe2e3+aKQ5pzH/ceR1/WdkGfirxnHwV/twOfvkOcAe9fUtNK7jwLuB1/S4PzqfP5+scL6y8d5eZI/1llOkMYfs1Qrg2eSIs08CX6/ny2zy+vRF8pbJp3aRr+0ar98EjJLXucvI69BjyPK6q3oCsEXj9RFkw2kTstf29sZ7C4EvTHUMjkv7XDIYMFynX1DPk33Ia+CHyXvUu9m/WzZe70dew8+p3+1zyFsgNu02bzWdTer/jTvH8rjt/Xwv2ztB+jPI68JX6/SLyGfQdF1XqNOPBs4j625/JgMDn6/7/HymuHY2zrnmvn4LOcJyZp1+HlkObjDZOddYf22y/Ps34FvjPudcsmPiJ8Bzu91WYJtxx/aNwOMa+f0u9bzsYR88nawXHNmYdzHZGfBScqTDdj2kt149D+7sHLv1fLkJ2L7LNB4BPLa+fm79zt8I3AN8cdwxeMJk50jnOGt+n2Qd6/Zxy+xD1teP7TKP7wdmN6Y/XNPtXJNeRnYCT1m3nOh4Iut995AdjJ15Z9Ztnmp7m8fMHuSIkK9Q60RkgO96spx9dhf524xsp21cp4fI0emnALfUeV8ly9qurnWTfNajyY7ea+r0rp1jvMv1X0heK95bt/nZZGfqG8i6x33dHoddf2abia3ov3oCvBHYt8vlHzVu+tj6RX6arIRtTF7I39vNZ9f/G5EVpTPq9IvISv9TGtNvALaeIr155G0d+9Xp44GF9fXJNZ+vJAuyHwNP6CZ/9fV69cDZmCys92+895I6b85U2zzF5x1EVupOIhtEX2CKiicZ6Llj/IlL3n7yOMYat+cD547frknSPYSsGH6fLPzfW0+k15GF8G3dHjONNHclL1Jvasw7E7iLvF3h8dSCvsv0TiQvfE+r028lC+3HkwXuz4Ade0jv2Po9bVen59btfO3yjv8pjpmhzjFB9qicy9hF8EiyYNum2/w1tvkEssH3ETII1ymIj6rbP6+H7X0RGQDagKwwXNI4X26nh8KRjPxvTwZC1q77+901T/9NDg3t+/xYzmeuO9V5V8/dY+uxfDc1mEA2Ij9OBiB6aoA2tvdfgPPq9Lb1uLsUOKCP9M4gy733A2eRDZ476vQFZCNy0jJrgjR3qdt4EHmR/nY9tw8khz9fRiNwupw0diIru6dSg7N1/25cv4NO4Hp/4D1d5Kl5jiys5+0RNc2DycrICTW/GzJJo2JcWueSw8KvA/as8zrPIXlKn8fXGWQDu9MoexlZIX4fOeru53RZxpCVj3eSZeiGZND/m+Q5Pb9O30Uj+LWcdDplyHPIQMcH6vm6WZ2/Kdn78wVyRNGk5f247/Bgshx4Bxlk7qQ5t+7f71HLiC63+Wzy/O+UqTsBHyUrYy8ny69te0jvpLo/zyZv5+kEHM5pLDNlUJMsp35E1iveC+xV57+zzt+UrBsdCDy9i/QOAf4T+HRj3svIIP87GAsG30R3waQ5ZPn+/Dp9KPC6+vpIMnj93voZP6GHcoG8TvwTtWFNXqdmkAHYW8hz/Uk9nB9fqd/bSXXeM+ox+bf1O5y0Q2eCNE8nGxBvIHuBn0d28Lyvn+2d4rOuIkcm9Fof3Kfu863r93cmeXvpC8i61+502VAmy+bvkuXW7nXeBeQ18/yaxyO6SGd8IORJZAfM2ePmb0etI4xfZznpvpQMml/YOAYvIuv8O9bpR/fx3Z9GjgB62bj559TPek4320uWKZ3OybXJIPVX6zn0OPIa2tXxQtaFvktewxeRDdBtyPrbm+syx5N1o0kbomSddmE9B04lRzA8igwEXDxu2b2ZutxvHn+7AV9vTI8POLyUHuo09Vg+jbwubUfWGV5H1p1eSNaDH9/DsXds3QfbkIGBm6ltFPLafg09dJoAzwJObUy/Bziuvj6brNf0VJdezudsTl6f7iA7qLtq05GjKM/v7EOyjfSNxnE5r9u0espv2wmuyL96Ul8IvKSLZXckhym+fNz8WXUnbdr4om+sB2o3hdnMerLdUA/E+fX/ErJicQ8wf4o0Nqp/r63rPIXs5XlJfX9TsmfzCnIkRS+N0LPJHrGvk5Wx/erJ98b63d1KFxWILj9rn3qQfokpIpNko/p2stfkJzQCDixbMJ1M3i/V7cVvS3JI/vZ1+gwyev02svfnfKa4ECwn3WFyqOXNZEOqU2n+OD1G/Mgh1z8kG2JX1n20HlnB+QgZDV1u4dj8jhr5+BrZAJvZWGY+2QPcdaCmLvdyshJ2CzliYxuyov4Bsjfz+/RY+JCV/+92jguysff5+v1dS14cd5pqe+vrw8mK/mX1nHgBeWH9Cnkh+BqwQ4/748dkxegDNHoCyeF4p5M9o5t1+x1O8XmbUXtLu9zeHcie5I+SlepOEO6Z5Ll9Alnh7rphVqcvIxtTnYDPY8jRIB+p3+ekPQGNdI4mz/s59fu/tnEuvoysCEx5/k6Qv83q9v6MbNS+Fnh1fW9/aiN6kvQ2JoPIXyPP+5OYoPFANoZup8uREnWd/eox+xbGegJnkBfuG+ihjCHL+y+Tw5lPIRvtT63pvY4MEHTVM9hI82lkubpFnX583b8HkUHr8+ii56N5DJC3xFxc87RBzeN3yGvIB4GDJ0nnEePy9tN6vHyULAv/jrFAWpA9SxdOkbeZwAvr613IUSWPJc/hH5HXlq0ay89lko4EGhXc+vmLGKsA702ONtyULB9fSm9lzMJ6LD6dvP7+XWO//Bk4q8t01q7/t6vHzDdoNK7Ja/odTDE6orH8BmQ5v5As/z61nOWOrft6uaN0GstuRV53LyPrHIdRG2X18x5LNvDP7eU7rOu/gLxuHkOee98hy/2tyHpct8G9E8igxTyyoXwbY4HXA8hA82Y95m13cvTWvjXNv6v7e3syaPOqXrd3edtB9mLfTdaNJq1fjlv3bLI8vYYMFr6QHAV6Tv0ep6zLMFb3eDQ5emYvchTuldRANTkq8P8yFghbp4v09ibrpcfVtLcjg3Fn9Pk9HVT38UyywX11471LyGH769BdPb+Tx60ZC74dSTbqdh+37DrNdSZJ8wByJMMPyCD1ENkpeAPw7/X8eXKP2/x2MtBzfJ3esB6PN5Hn+XfoPsD8erL++CNqZylZ/t1NPrCz2zxtwdh1qNOZ+hXgusYyl5IjYqYM7rHseXxSPZ4/SwYIdiGv5+8my8ZbmToQN9R4/dS6zlAj7yfW/XQ5eU3tpVNx57pPbgdOq/NeR5ZhF9Z90lVZ3eXnvYK83WPKAFXjmL6e7Ix9Up1ej2wH/5S8BbaVvP3V56+ohFdIZrMR9Gim7kHfiIwYXlUPzDMa7zUrU/uRw0UO6mZHkYXh0npAPpNsOJ1A9ljuWA/c5UasahrzGKucb0VegN9PFjgfIyO8+5IXrZ4isPWA+UHN54HkUMNDyF7MheQFf+CL37jPfCRTDOery21Kve2C7KH+OWPRw2bB/g56C65sSjZq967TnV70zwJHNb/7qfZv/f8kshEwRFaUPkc2LJ5BVkgXd3NiN9I9jGysz2rso/eSvQudIbYzpkijecxu0nj9eerQw8a8bemtcDycvBCsRfYAfLNxbB5KVlZ6Da4Mkz0JQVYIn1eP80eSFbRJG44se3GZTRaonWGQzycj40c0lun6Vpa6f79Z8/UKsvLwAZYNfm1CBoB6ugVgOZ83g+wpWO7tLOO2txME3YhsiL6fOoS05vkophg1Ne543psMZHZG1HyIrCBv1Dhe5vWwPRvWY3g3skz5CmOVrZ5HXNT1jiV7Ak8jG8hb1OnPkb1dkwYZGumsS5bBjybL5YtpBBwYa7hdSA00sJxyoR7/nSDPSWRFptPjvVfN31vr/n0ujaGiU+RxP7IR+vpx238nWe6vRQ+NnsZ+fgp5/r6CLOfvIEcL7FLfnzKQ1Ejr8WRZvBHZ2/Z+xgIOu5A9M6dNks6WZBBr5zp9DDly6ECyQX8cWel6G2O9K8eSAcVHTbJPXkQGK06uacxvpHkI2WlwKfVcq5/7K/761r0gK1i/At7aOe/Iyv61dd/eWtObsmNjOXl9G9mxcXxNawZj5f129DbkejvqyDMy4PAaGqPWyFui9u4hvc4onM3JiucnG+/NIBtFP+zsv6mOl8b5spBs1H2eLBf2IYMFhzJFT+gEaQ+T1/Kd63f5jbqPd6rH96S3e43L2wLyGrQped29pebt+9SRgPQ4rJnsvXwecGadnksG4N5KD/WXHj/zhF7SrsfMrdSAEXntvIosuzchg5Dd9obuS5ZzH2zMeyUZ/D+kTr+ZfH7PlI2p+v39mixXPkbW2XYmAzX/S5fBuEZ6e5HXxmeT5cNXGufb/Pp/ix7TPKged9eTZcJW9Tv8JTWo0kUanTJ1bTJg2el8uYIM1gyTt61dBPzDRMfvFOk/gbw+/SuN4HnjcyetG407T4bITqbbWDZYvBkZBLqoyzw9lQwE/H1Nq1M/+CL5jIrOcu9lirrquPxtUvdtp3x/O1nP7FxnNmbqW807tzo8qk6fTAZTXj9uub3IgFwvgb0gO9kuJa+TPySvWY8i64AfoaVRTvXzNiXP765ub2bZtsOVZCdxZ8TY+uTow7lt5e+vPn9FJbwq/8gLZmdIyK71ROzc9tA5CdephdOBzfnjD576v9nYezNjQ2KOJIMaZ9DFfX6MVV43Jxuvu9Wd/CrywncP2Zt3fT0Qeh22fhxwfmP6yeT9Qa3ee9Njnp5OPvX+L9tfXx9DBhz2r9PbkhXtnu6lq+ueQw5j3KlOP5Psmb6KLgIhjXQOJqN7H6sn8RlkBfv6WnBc1MhvN9HxR9Z98B/UXpQ6//Ba8JxZt7nbC8vCul2XUXtSyQbZrQPsn33Jxv9rWbbhuEuf6e1FVrh+Xc+NT5HDyO5sHpuTrN88184hL1pLqMPSyAvO88lGwfHd7ou63HyygvhEsrLz45rfa8iGWec73ZPsbe76eQjL+by/PDegy+XPqPm4kCz41yaDPe8jR+n8kB7uJyYr+T8ie2a/DZxc519Jljc9PW+ErCy9ghz58QeWvU/yJeRw2p6edUEGVDojGS6p58XTyPJ5V+qD23pIb73G6xPJgMMpdXqLcctOWOaTlcrLqQ0QMsj4Z8Z6p9cmG/dXAG+YIj/jR2+sV9f7HMs+b+AUsmLWdXnV3CYy0HIBWTZ0AkuXMzZarttz5MC6P/6OHOq6Cdkwu4i89m1EXrt+SjYkJ/oO59Vz6vWdfVf351WMVfw/Vac77+9Dd72sr6znwwlkJe59jPXQfLR+bme49L4THTuMjQ7btm7Hm8hy+Bk1vR3rPj6HOrKmh/2xD1kJfAVZ5jUbEGdQR2Z0kU6n7rF7/a4+TNYb5pHDxF9Fj88WWM7njL/3t/Mw1q6f9UM95+oxeCo5suhf6vZeUfdJL5X2c+r58WGyMbARY4HC55H1hrldpnU6GTTaluzx/hxjje/PkuVtr506x5G/PvZJcnThrnX+bLLu8EamuN2rz33V60i2R5Dl5+GNeW8Hrqivu3q+EVmHuZtsHP4H8KrGe68jO906tzC9aap9Xc+1N1A7g8hG7knUZ6fVY37Se+THHX8bkw28S8ie+e813juNvAb0+oyGx9bzd3eyrD+rHjuPrMf13Uxxm2ojrUPIesz3GOvtjnp8X0t2om5ANpyvm2o/1/WPJK/HnbLvCMaeY7KQLPsnrVuO+w6b16K3kvWG2XV6a/LcnrQzgWXrbh8knyG2/7hlvkB99kgvxzs5YvIXjKtH1rx+nR6fJ0bW/15cX7+ILKde3Esa49Lr1BVmkKNJXlLPm0XU+seK+KPL+lY9Zq8mR4d1RnF8vO6PgZ4f0XVeV8aHrMw/GgECxioVnYDDWXX68fRwKwGwW+P1kcBNjemjyQreVPfHb07e/9QplF9N9j49kayIvYLsddyti/wEE/RUkRXFG1n2gZCX0vJohh73x2FkVPSgxrzOPnoh2YC6jOyx6euhlWSPz9tqGueTQ5h2Iy+03d7PuRHZ490ZIbFtLTSOJCtkN9fP6OqiRV7kriIr6CfVfd18aOUh9DCcimx83lG369Xkhb/zjI8fAp/r9twYdxztSwakbm7MP4UMamw0fp1J0u6cayeQt4e8i7wQdEZ0HFbz3G3D+0AyyLMxefG8o7FvNiV7WSatEI9L71CyYfGEOv16xkYMnE1WEjujJzZg8EDDemQjYd1uvsO6jd8ke3ZuJIfvdRrJzyJ7CZZ720kj351g0frkxWRTMqj6A5atUFxND0M2yUbEDxmrgLyjnl/DZKXnn+mxR49s0F3IWON46/o9vIMub+mY7DgnrwEn1ePwfWSAZPsu98cjqaNC6vSTyXt2T2nkfQ8mOYdZtrL0TDKIsjFZIfkoWd43h3T29BA5xu5Bv4hxjViy8tnrENBtySDUY8my+ReMjbR5GlmZ7wQH/oYJGnyMNQoPJ3uQL2fsIWa3kNejnckew0l7zidI+9lkQ/sbZOX9VLIC/xqy9/rrUx2D4/c92UD8ZxrPuqnzX0xWFHu5dWIzspK9DzmC6guM1TleSDaSexm2fiA5au/vyeHhb6/5nUdem85jilFxXea7c+/vnWQ9acqRU411zybLuSuAfeq8hfXY7udZMM9mbHTdt4EP1dcbkZ0BP2eKcrCR1qFk4GxunZ5FBkL2IgMG19DFbSLj0nwGeV2f19j+24AFjc/o+rrU1h/LljVzGRuF9Vqybtl56Omx5JDzbm+Ze1Ldl51r5R7kdaD5oNOe7z8n60VfZOx6tUM9f7eeaJsmSWeH+v9kMshzHhkE+ps67yfdHi/NzyQDjs1e+C3q93BYnZ7XZXrbk8G855OjO7/IsqMyr2ZstMP6dPeMlLNqefAasq7bqQd2ytyvM0W9d9zxclY9hq9h7PlBF5Bl62vJdsuk58m49E4ng04X1PNvt3HLXklvD7beq55zO5DBxhuoI4rq+39LD7f5Mnarwy8Z67A6jmyH9DySrR5rb2ZshM+z67G4FtmR9y0yaD/wLbn9/JF18u+S141f1/OjUx7cxP/f3pmHy1FW+f9zkpCQsENAWYMIagYRlEXCPg5bCCQRFAIiqyiQALJkFH/DvkVBNllkGVB2EdlkxwUVWUSMKJvDjODgMI4bOjOOiML5/fE9la7byb23q2/frk5yPs/Tz73dXV31VtVb73ves8ItXWlHHSff5QtdCD+boonq0uhkmw7wm7KwugKy1t0GQgAAIABJREFUcl5Iw/J5NX21a0u32JZiwVgIcLPiId8wBpoTkVA8bqCOWT4e0qCdjBZ3Y+KhvBG5Qn48znVIC6c2r3tZy3k8sgTMl1EYLfJ+Q4tKgQGOt2yc86eQa9nmMdBViWW9m9Ikgrwv5sT/E2gM8INlbd4DCeobRb85Ju7t45Qm6QrntgUSag6L9+OQsuKGUv+eUGF/s6K/XIEUKbOjn+wU96pSssrYZ2GxHIk0xafSyKD+sbgeA+VomETDJXVtpOz4Zun7w5Db8wfifZV49o3iOSsEsBWQBf3nyDo7l0aC10rVLAbrUy1utwxSTq2AJv77kGD9OKVEQy3s43akWBgdry8hF/jv0ljw7Ur1uOmxse+d0MLkMKR4ezme35tb6S8LumdI8L2Vhjvf+sirqCWrLf0IzfQVfi5AMbEDJi5r3hcSQB6lkfhuczRWVXXxPRYtnK6I6zUJKRyuQJU2WgrBaNrnAcwfg14kBdsRLchbieMcWfp/peh/e6IFRaFY2Ak914XraSuxyQ/HeT+NFMCro4XYfdG2QZPINe1zldjnxHh/GBLoTkVKgTsoZYpvoU8cgWSBI5Fs8ByNhHLvRYrOyi6vSIi9Ov7fAwnw34x7NZiysFytYxxSFhYx8ZuiMftCNE68nUHywFRsd8uxv6XfzIx7+TY0PrxEIzzpKDT2DKiwRuNUsWDcFs2v+yNF0v00vMPeFq8qRqJDS/e0XAHpRiR8tyxzIFlwCaQw/yEyMhX7nIXkm7a8AYd435qVZ7PRWPNMPB/7Ic/CW5C88HTFe/xBNAaeSUNu3RTJC5+u2NZN0Py0LlqEnwqcFN+th+apCRX2Nymu+ydQqNF30VgzDRmHvjTYM9d8HWlYqJekSV5DivHZ8f+gcgIKCfw+cEG8XwbJRndQsdpaaZ8bxLmNjn73k+jPh5WOUaXSy3Q0f6wT9+MSGt7eB8RnVRSun4jrVhglPoMUPmuiMbvlPoNkx/dEn70yPhuL5qNbKHnYVNxnc6hDYVD4GBpfqyr914nfPovG0WJcfH+5T9XxQvLeecgL8Eg0F52DPOeLBK8tK5eH1Ja6LkKXL3ixIJuJ3GH7TYJBX4GkcFEdFw/elXGzZiKXp5bcqJr2Pxm5YBUD95Fo8nofUhYMGK+LtPWFG9z+8dvJSNt3dww2s5EQf0eVgaLD17wYvGfGdbseZcDeo7TNdlSc/Fo89t8jC12/wkSpfevQSJh3QvxuTLyfgQbicibsQRU3McAWSRpHI6HnPLRY+TaDaDmb+uCyKKfA9WgQf0/puwcYQGnWz74PQzH76yCL3nnx+bFoAXkJFUroxG/XQoJmEV40Ci3UHkRW5bsH64exj1VpWIx2RJbz8mR/NAp9GDvQ9VvAviciweMTSAv+LaRdfxhZXvtNdDecz8YCPp+AJoGilNo98Wophh8tEu+moWHfD4VmFQuAbZB7c6U+E7/9OBIa7kQx07OQ8DWWFqyrTX16J2RhK0o0fhZZb0ciheygyga0+CzC0vpTOIyIZ+0VGqFcxuCL5b+jMQZMQR4ne8X7raKvt2SpQF4pd8X/n0OKrcvRWDCKpqSGLd6L/mLQH6ERgz5gvhFKITRovDw0nrmfo7w0xUJqcyQ4DZqPI67tGLRI/kh8tm48e5chzxWj4d1X5RleAS16ton3RW6ee5DioOV9Ik+c7yCPuN9F/9sNjYcnxTYtGRBi243pm/PlXkLxQSMsZ7Byo4Vb9d+VPrsEKRgK+WUymi+Pp7Eo6kQC25Zif+lrQBgTfWYlNHfcSqMk5ZTYZlBPxegfD6D57Ta0uH2cvtnrj0WLg5a84kq/mxz34p2lz3ZFsly74UojkFfchfFsFHJEW1WCOnDvRpXatRla3I5CeVM+hxZWayIl7u60nv/mnfH70WgMvC5+XygcN6OFEqul/W0bfff22Nc5SPFwRbR5LoOUt2za3+h4fh9B89I0NC/9hIaStJJ3HFKS3o3Gg33QOPsVpDTbPtq/dYv7Kjw6T6LkcYWMWgcSiQIHa2OcZ1FWtCjJ+NZo6zfi/TFI8b9/C/srP8ProvVDoQwpQscuiedwxGD7a9p32SixSjwTJ6P58jY0frecB6b02X5IgbZl6ZpMjX60woJ+08+++wt1eIKGx+JQStS+AxkVz0DrzK/QQiLv4XohBcP5NAoZ3F/0ASQXnkHFcXBI7anjItR04VeNB2Ge0DnI9ocjTelXiViy+Hw2Wti/SsWEM6V9NCscPhUP02Bl3VZCC8V3x/ncQAjB8f1VwNdL7yvFT3foOq9b/h8JcIVVdQoKqSjuwYoMg9dFXJsJLd6HIsP8XfFwXoQmleOQF0phWaoy6E6nSdGDJpx1q9wT+roIno5i769Gi4JpSChrKba29PckZJ0+Fk2sS6JJplhctOW+joT1HxFJQOOz+2LwHcjVfBUauTamR/84tXR/LqNvqbjKoTYoGdosJNTsHs/P4XG8ljJJD8eLhofJVWiiWi6e73chr5p/prVs8MVi5N1IGfobJMitgxQ0P47jPEcoItpo65LIolUs6j4SfbpqYrVDYkz4DlLkbYaUIFehRe2jDO4COhoJq9cwiMIhvitKQfan5HkvUXIt+sUzaAFUjFO7xHXdv7gWAxyr2TtifaRQOTDu7Ro0ancPWqZwAfsfLAb9bgZXWI9Dc9uHkBDyTPz2DKSk/m30myNoI0M1EjA/R2NhshFa1J9MxdKCTfstPMTKuXluolo+gGWRkmJVpOi/H82jVyMB+weDXb+m/b0PLeieRbLBxLjXh1XYR1kRtw0Nz4gt4n4UiZXXi3v/EBWz1rfQhirz0jQ0B22NlA4P0VgQfRN5kLZ8n+NZ/m8a8ez/jObkPZGgPJcKrvBN9/p0ZJXfFSk4n6BCDpjYT1kWPDs+OwUJ8ltT32KiOTT3/XH9i+furWisnVFhn0U45CQUNnAmUuwV+clmUNHIhjx351UJiGt2Io0cQhMpVaZpYX9FNYz1kbfLndFPPo4WeSdQ0UMRKQzviPM7HM1HZyEjyDXRR1uqXoSUqpcCB8T709CYX8hzS9OiNTnuQ5Fs8Uc0SjcfBFwf/++OxrAqYblvib8z0fhfzutRhMG2rHAt/bYwStxBwyhxGlJeVRlXP4Lmi8PjOd4bzZ9FyOUSVBtjBgt1+B5DUDSUjjOGRqW5lpMAd/pV6msHofXlemg9814aniFd8WiY16a6LkYNF38J+pbami9+vfT/ZDRIT6QRH1ku27IibdTqbTre5Lj58+qNt/CbZdAC7makNbsQOLr0/YgYdJZqPqcuXePl0eRUWGYNWbnWo7GoOwlNCAMmAOpCW/8uBsV1kGLhMRplz2YgQWfbIVyH05HwvgONRH0tx4fS10VwPbQ4m4Msmg+ixUGVuLx3IC3rl5FweBONhdostIhpOVllP8fbBVkWDkIL+bsGG9Di3B6IfnIrshx/m0Yyvp2Q1WtW8zm10b7CJXcTNMH+Q5f73Go0tOszaXiYPAl8IT4/E03UT1Eh6RES4H6MFu8nIcXKlLjnmyMvoo07cA1HIOVXS7HT9LWkbE3fLOGfRwJtEbO6Ci0Iskg5ti5SRJ1HQ9myoDw25eP3lxByB7ToPAuNq8sjt8jLaAiMRX9uKY8JJe+IeD+HhqDzGRTaUakEFh2MQadhQb6XRpKxjyIh6WK0IP8kjUTL/SlqCiXmetHPlo1+dzZa4I1AHju3VenP/RxrDSS43k8jN892bexnDFoAfbvUp/+AxsEqlW1morn4+GjPl+M+P43y4LSkEChdw03j/v4y7sHSaCz9ChornkfyyFnAvkO5lhWvV3kemYHCLeagOe1ApFh6D1rwzWmjX68bfW9u9JkV0fx7J3q+2/Z8jGfkUOQBc2PVPsj8suAjwGXx3RfQeN11o06pfbuhXBsrorH+yhgnikXpibSYlDS2L2TkEfE8nxv3dCSy9t9ItVxJxe/+j0b+kqIyw9VtnvOaaHz+YfS/T9DIKXEwFfLUxG/WQh45Z8X7ourO9XGscv8fcByM/8ehRfeFNEr1nhJ9sNUylOX9zZdsMdr1Q6RcforWS8WPoFF+tyhTun/sZ1ppu0plYEu/azZK7IvkuSqJ2mciefwoNE8+Gv17XzRnTWqjXV0Jdeivf3TzxfzhRY/GPd6HhifQsFTMGbBddV+YYbzggw4Q/Wy7Ttys05q2+TYheHWwjdPQBFulIsFsVM94NhLinkFWzLci7d8TtBHe0aHzGYkEufcBn43PrkQCQ5GxeirSFFeaEIahT7wdTXiTkTWr8L7Ygg4ID2hheQRaXA2qGGj6bbOL4HRKLoKEu3KFc52FEn99HmmLf0fDinRADMAtWwcHOe62MYjfQ+sleQrLViGMrI8WyyfE+x3oQNKt6J8bowlr2OoJ93Ps1ZFw+gmk+DwOWaYLD5MxNKxKY6s+w7HfK0rvP4rK+n6EzuahGIcEvIkVf7cXWmT/gr5WlHOQp1irsbXFNdoOCXM/jn3M5+FAQwmx9IL2j5Qb74z/z4px4NbS9/sihUPxrPRrSWHB3hH3IeXgWCQE/xEJnT+mvaRqHYtBj9/ugDz0ijjkUUggmQMc1cLvi0Xybig85w5khfsgUnhdFu+fpo1kgf0ccxlklTqGNpXBsZ/1kDVrA6QkvY5q8eLT4/croLHq9Ohnb4lzf5JqHhebUVqMxP28MO7JMnGvJkS/f4EuzZ/0nUcmxHNczJX7oDnpOaQE+tdWn+N+jjUVKTH/Ic73lIGeuYr7Hk31igT9yYLfQzLO0rTp3drhezQ5+sRScQ2vQN4h/w8togerDlE8xyvGGHVUvB+B5KGvx5gwihbm4dL+lqOhXD+Qvt5i2xDhcrS5QEMKw3uRMfD5IV7Ds5BcVPbM/RoVlJlxrXaJ/8eiBd7FNBQhZzJIydYF7LPfZItIRvwwFT114rcfRx6QRULIj8Z17EhIKRWMEjTm9KLffJFSPhokNxQ5G2bSYihQP8fqqVCHTr+Yf+0wFcn9TyEZZTwVlcEda1vdF6eDF7kQLOfTUNFX6Nykv44VD/bXY5B4vnxTkOC03TC0u5KrEprwt0fa7D1j0L4TCUuDJqHqwn1YCy0470BC9xLx/3XI8vOjoQwWHWjfligGbK8YbJ+j4dFQXMvKCdsGON44qrl7DeYieCIVFo8x2FyOlCuHIZeqq1HM1kXRZzqa1yPOuYomu7Bs/YhGrPfKyEW0ZXfkFo+1FLHQ6+YEgxREB8TAf1A8D9+hr4fJEWhB2YrlvNkzazsUG78Gjcn7VqT4qZRxveqxB+jHM0rvH0BuvrORMPGB0nenM7gHTFmJMCn6xvZI+Lg8+vI8D4fS/8vHsedbiKMF54PxPHwLKWZ+SCgNYpsiaVS/yh8G9o74Ytx3Q4qHM9p93uhgDHrp99ORYqRw1S8skf1aPijFztNI5lp4zXwMWaPeixbJ76fmOamfcxiDxsIHkRBfNU/N/sh9+YDoX4UyfUL8rVpWdkfgDaLeO7IQPkS4SsdnE6O9Xbme9FU0FNbGZ+Mel40HLyMFUyfckHeO+/Gjqvekw+c+mCzYVa+4Ftq7S9ybcWg+PQyFogxo8aaxwJscfXkrVD7y0NI2lyIPnpZLqEe/+DYyfu2HQqn2Ql47/4xkwt06cN6rICPbY7ReIaLsSbQ38kJbKsaDp5HL/cS47y3lN0Jy+Y5Iriq8wZZGniGPUcG7pLTP/pItrhXP4IkV97clks2LufFA5HFSVCbYmw6FNdOGUQIpApZAHoT/WPr8PcBVnWhX7K8nQh06/WLwtcNJtbav7gvUoYtcDB6rU4qNis/KQufj9JOghIaLahHfe0oMHNORO89cakgANMA5b4wsCTPiwVmWQZKCdaFNmwLnx/+bIc3w4UjQ3hItKDtiQW+zfxQVKq5DngJPIs3r/khxUzk+eRja2jEXwXge/r0YqGOQ3QdNqmfQott6F899t7g/u8akeEMd/WUY+99+MZHeF+PLb4HD47sDqOhhQqO819R4xq5DVtXt0YJ8XomjGs55CrKqzYj3jyGr79rIGn8JpXK4g+xrtegbS5T2XcROL4kWtg8SluDS75ZH7uf95kag4VVTZPOejPIgHFXapt9FFBW8I2ixDOoAx+pIDPoC9rtLjIX7t7DtKKSU2RzlCJmDFttlF9zPAzfW0e8qnvcSaLxtpxrItijv0vdKnx0Z595WOUrk6fgzGgrXMchCVU4K3JZ78xCv0zTkobgeCpm4ACk3CyXpnnRQNkKK5tpkGRYyWbDU7in0LVfbannLzZFS9O/j/abA75FH5NYxtrbsdh3jwgPIa2gK8rSYhTwdDkEK0yGHRTYds2ry0N3iWp0c97NQtp4JvIbCOt8zUBtpKPW3irFglRiTf0KEPNAIAx1KBajmZIu3UyHZIo1KKheheXcLGnPpeWghOqQQt4GOP8D38wwSyNDyr0guPwMppQ6K7z6CjAEdKR/ZqT7Xay86HF7U8fbVfYHauKD9ZR5fFikTZjVvG530QQbIJEtfF9WRpc9OQZavrse4tHAtNkSuXx21/lY4frN1dTxaMB0Q7ychF99Te+BabYZceosYrbcjweGmGLzPoVHyp/bBiA65CCLr2yulQX0EsqyfRRvJFrtw3oVl64dUdNXv5VdMmE8gLf1FyEPlXOR+fiktepiUBIj3I8HjNLTAPQ4tvE9DgtJjdLnKxgLaOjnu5TS0SF4FuflNRB4OZzNImd/Yz7bI+rQCcvXdBFlTNy9tc3k8x4Ub+jJooTZg9nD6xosXFSeKUJtB4+Jp3TviAjqg2GOIMegD7HdqjN2rMXhG842A/4zXu5FQ86nStd86+nilBcDC9KJhtTwHLbz3QwqbIXkd0FD8HBjvi+e9Y6FQFdtTKKwLN+ZijPkC8uZpS7HSyy8WQlmw1PbpKEyr30oC9PUSWxKFtb7Y9PmmSP64l2pVIlZHHqzlstWT0DwwKY63F5JrJtd0jd4V49MqSGH/FI1qI4aUho/RqCgxUH63TZGBc5vSZ3sjQ8KZyDti2zbb2XayxaY2Fuc2ChkjLgC2is+mIk+8rlv4kSLqRaREuZK+3rdfQnLrF5BSqGefuV570cHwoo62q+4GVLiArZQ627L0fzFJL4vcuQYTOhfkoro7bWZv7+J1eTd1a6x0bwpt+geQ9nQ8WlhsEwNJ3V4XOyA31XkxzzHQnt20Xe2KhqbrWslFsJ/9TInJvqxwqOTmW8N519pfhuGcTqURGz8aJd+7GVn5x1NhIRoCzrzs2MhbYC6lGtZUqEc/zOc9JQSuN5ES5NtIwXcDLSTFLe1ndAhFM2mUD7sfKafeF2P3erGtxdjdcolPGl41OyHlyPdoMa8CQ/SOaPO6Vo5Bb2GfLT1zcf0fRnHiWyAFyGXIs6ZQoNXqIdaNV5z3J1DOlWvoUAnn6H+F4qcWJUNTewqFdWH9HUVD0VJbDflhPN+FUhYstbXf0NwYNw6JvxvRKI35Q0o5f2LbJamQbJxGeOIBSAG7P40SwufQMEItj3INdLwS2QBtK9YDmyFl3sUonONRGuW2d6FRNvMMpDweTd+FexEONwkppX+AFAsXNB1vGzTHbzeENnci2eKCKqmcgKptfCXa39WqBE3t2wF5F18R78vet3NQ3pQhJeNfHF90aO3Q0TbV3YAWL9yApc5gvnJj5cHhA5QsYAMco9lFtbBELvRu3MNwP8rX930oXOICZNEajxYVRQzvGGrM1tzU7mnIVasQmraJwfYtrUymNbZ7yBZCJEC9TNR/z1fX7+F0Spb3+OyRGG9aWoiWBKajkLv1MTSEuQnx2XnlbXvhhRK+zUWKgpFosdqyoBlj+PrIw+EaJMyui0LIvk8p8VjpN5Utrkhx8dOYoKu4DQ/JO2JhfCE3363iehXlgT+GlGqb9FofHOZrsUQnxuimffaUspWGwrqscOipNnbwXBdJWZBGFaD9UHWD52mEgK2GvKUuanPfy6AQwZPj/UHI6HR+jN8vULLw1zE2IEXDXSi3woeQB8j28d0kpCQtJyZc4CKXhmfXSzEvbYoUjkc0bdepEJEqyRYHqqr3CHB5fLcFUkTUlhel1M5pKE9Is/ftvMp2+Wr72vaMd2EhvPY0ZjYSJbyYjRKaHOfub5jZCHd/s2nb+T6rcJxVUcefirLynuXuPxla6xctzMw8Oo2ZLenur5nZu5BwfQrSCH8EKYh2c/c/1tfa+TGz3VAM3b2oL33N3e+qt1Xdwcx2AP7N3X9ed1sWN8xseTR+ObKYjEVW0Y+7+68G+a25u5vZyu7+m/jsAGQBOAX4gbv/1cwmAGu4+/eH8VTawsx2QpakY9z95oq/PQRZwiYj69OeKCTqGmRpGu3ufzYzA/AhTGpmtkrs4jdt/HY3NP7NRlap44D93P3FdtvT65jZrkjRfD1SPhzh7s/U26pkODCzyShc6Rh3/2rd7RlOFjVZ0MzGI2+U89HC/wHkjr+Wu/9vjJ2rIu+kF9394Bb2WZYFR6LcOZ9B89EcM9sXKZifR4lOv2FmI939jWE4xUExsx2R3PdJNBedg8Ly/g/lrfjHVmRBM1sKXb9VUCLNb5rZzsjb7lF3P7/D7R6HQk8ec/fnBtiufD/WQUa0Xdz9hNI230VJaL/XyTYOFTObgkJ7z3T3m8xsBPKq+Z+am5Z0iJ5XNhTKAzPbDrmybYOSfn3a3f9WVi4UA5mZLY1cR55u85ijAdz99c6cxaKHmR2DtKNvRRPx3Wb27vhsF+TxsJG7/77GZi4QM5uK4sSuc/dzO7FISZLBMLPV0Bi2O/A34Fh3/2mLv90VJVH6KfB9d78tFuEfRPGcD7v7X4en5Z2hVWVXWWiK9ysj188r3f0nZrY7cim9B7iml8bpEDrPBv4EHLw4LLzNbBKyvH3V3e+vuz3J8LG4KawXJVnQzD6LPBjORXHws4BPoxDjfzGztWLTldx9bov73AL43xiXR6Kw3tOAb7j7hWZ2EMq38yRwS91zlJlNR0lOZ6FQj01QGMmL7j63ee4ZYD9jkYx7KSqN+lUzOwrNS9Pd/T863O6W2hXbHoZk8FuA45FHyX/Fd5eh5L0PdbJ9naCkzDza3W+puz1JZ+lZZUOTEqFINPgxpI3dELmBHVV4OKBzeSMsiDej+Oinamr+Ik1oIU9BlsYPI432w+7+5Vi4G3KT/kWNzRyQ0HJfhRK53Vp3e5LFh7CMmLv/b4vbb4esx3sAn0UWi5tDmJuFFA4fcvdXh6nJXcfMtkbhF+eE5e0zKAnXzvH97mjR03Nj/FC8IxZWzGxUKP9bFoqTJBl+Ss/mhqgyxBLI4+qnMa4ejULzjkOVUPq1nsf+Ck+7t6HF7I4oR8tTZrYEsvD/I1qIXxj7XgkZpf57mE6zZUJxfyqaW27owL4uQB4hU9G64xtDb2Xb7ZmKQoB2dfd/N7NTUNjM0SjU8gAUctiTysLFTZm5ONGTyoawAG4M3BfuwVNQopXZZrYkikH6HPAccun7W/xueaTNO9Xdv1tT8xc5FmBpPATlZDg03u+OtNk7uPsrNTWzMjmwJb1Kk0vkocB3ULKkU5AL6HTgHne/wMzWdPeX62ttZ2g653VRxvAXUMWdOei8b3D3u+trZZIkycKFmW2PFqGnoioHS6KwgZ+Y2eEo78DN7n5vi/srvEMno7C2w4A93f3pONZk5MnwaMjsY3tJGW5m05Db/vbAr7zN0OvY1yRkCL3J3R/sUBPbbcuhKKHkmSVP70NRiMxaSMGyyHvbJb3HqLob0A/robq1S4el/L+AGWb2NXd/DJhrZi+ih+edwDNmtgxyq/1Ur8UjLcw0LQC2QomSngW2MLP3uvtcd7/VzPZAA9pCo2yoe2JIkv4Iy9FWyO3156jawRRklfhFCHsbm9na7v5SjU3tCCVr2SQUC/uqu+9sZuujRJgPoNKXv0PJuJIkSZIFYGZro+R/98VHOwD3u/s9wD1m9jngWjPb390vsUb+rUE9k8xsI6S02Dvc879gZpsBF0dOgH1RYtxHY8H7GvDa8Jxpe7j7HWb2SCe8z+I8nyiMnjXzC2Camb3T3X8Wn/0a+KW7n1Rju5LFnJ70bIB5sXLXorItX0La0xnAeejhOQO5wL8QCokPAi+7+xP1tHjRxsxmIo34TijJ3fHA/6CsvH9AWu7tFibPhiTpNUqL7s1R/OJcVDZyORQjejrKKn0Byvmw0CYta6aU7+BelGzwJXffp/TddsAj7n5nbY1MkiTpccIb+DLgcHe/08w+iryFT3P338U2zwDPoNwyLSfiM7OJqDThoyikbztkZPobMvj91t2/1cHTSVrEzJZFISwjkJywHEqIuY+7v1Bn25LFm55UNpjZB5A3w3iUeOpbqK73JihJ2p+Ay9z9a6XfjOoRzeIigZmt6JHcMayNFwNT3f2X8dkaSPGwGSplN8fbTMiZJEmDsBKdBXzG3R83ZZaegko/roPy1XzW3W+rsZkdJRTGNyJX3lvjs+8BP3X3w+N9kSw48wIkSZIMQCRDPA1Vx/kxylF1Iyr3PRIpDC5x98cr7ndpFPu/N/B5VDJyG+SNdmOn2p+0hy1ilVSSRYNeVTZ0rdRZMj+R4OwQZGWcgNyX93b3o01ZeP8aCYeWcvc/mdlYd/9znW1OkkWFyCVyH3BCxF4ugcbBQgkxzt1/vbAvukteHNsBKwNbozwU98X3E4FjUXmxVCQnSZIMwALya+0JnIjkuT+i8XRFpLT+dKs5Gvo51mh3f93MNkHex0e5+zeH0v6kc9giVEklWfgZUXcDYJ5Vq8ztqDbv+u7+deA2pKU7CBhZLGw96GpjF3HCY2FtpPn+HNJ+jwU+bGb/4O5/DkXDwSgpEKloSJLOEblEdgcOMrO9XeXCXkWeREu6+69ju4V67AtFw64oNO7fgZ8BXzSz1WOT1YG3A+MWMEckSZIkQUl5u0G8VnX3m1Ho3RUoceDBKJnh7kNRNARvmNnTHGpOAAAJK0lEQVTGyOv1/6Wiobdw99dT0ZD0Cj2RIDIGyHKps9+Y2a/QYnfnSEAIqhyQD88wEe5xR6Cs76PRpLSjuz9iZkcAV5vZefHdDOCjtTU2SRZhIoHVm8D14Q77f8DJ7v7bmpvWMWK8ORiYGa68j5vZeOB+M7sfeXMc5z1QLi1JkqSXCTl6MlLe3gDsZ2Y7uftNMZdca2bHRpja7zpwvDfM7Hlghru/uLB72iVJMnzUGkbRVOkgS531AGb2PuBm4PdowpqAknS+CrwB7IYSRH7JB6nHnCTJ0CiVGLvO3c9dlELHzGwpFC5yqrs/WLLM7Q/8CIXLPZlCbJIkycCY2QbA9ShZ+oYowfAoYHN3f97MZgD/4VmtLUmSLlObZ0OWOutZJqDwmmfd/fwokXQpymp8K5ATVZJ0icgk/hpwlZm9VCRPXBSIfC83A1ua2S/d/bmYD2YA97nKqi0SipUkSZJh5pfAXsAaKKxhvJldBPzYzDZw95tg/rwOSZIkw01tyoZQNPQpdRbC9D7AweVSZ3W1cXEjLI0TkWZ8HTM7Lf5/sJQhPieqJOki7v6AmR0I/FvdbRkGbgUOBS4zs++jZMBHFoqGJEmSZH5KBruVgb+5+6vAq5EH547Y7FvARsCayGs4lbdJknSd2sIostRZb1K65rOBI1EW422BP7v7qfW2LkmSRY1Qcm6Kara/VLUUW5IkyeKIme2GQo6fBl5w938ys32AHVGS9anAIe7+TMrRSZLURVc9GxZQ6uzXKPFZwceBY81slLv/zd3fhNTEdhk3sxVRnd5nkXfJZ1Bm+CRJko7i7n8CHqq7HUmSJAsLkedsH5TU+1cogfebwEnAksDGwBx3fwZSjk6SpD66WvoyS531PjEh/Tcw3d13QiUwV3D3l+ttWZIkSZIkyeKLibWAL6Ok3Q+7+7PAHqg88hx3v8rdZ0bOn5SlkySpla6GUUSps2uBs939kfjsJODDQLnUWSaErJmSF8pId3+j7vYkSZIkSZIsjjSHQUQen0OA44FH3P2vZjYBuB15PDyf3gxJkvQC3U4Q6cB4YCmYN3ieYmYvoVJnN2Sps96guP6paEiSJEmSJKmHkvFna2Bz4BXgFhSG/E/AaWb2qLv/wsw2d/e/1NneJEmSMt0Oo/gTUJQ6m1gqfTkD+LW7PxnbpaIhSZIkSZIkWawpVW+7CFgO2BK4D/gmcBVwJrBFbJuKhiRJeoo6Sl9mqbMkSZIkSZIkWQBmNh54e6k6zw7A6e7+1fh+NvAFd9/bzFalb7L1JEmSnqHrygZ3/w8zm4M0sm8Bbs9SZ0mSJEmSJMnijpktAXwMWN3MlnD3h1H48SbAV81sBDLcbRDV286tsblJkiQD0tUEkUmSJEmSJEmS9I+ZrQIcCYwGLgXeBO4BLnf3CyIE+XzgQ8AvM/w4SZJepY4wiiRJkiRJkiRJSpQSpE8E3gO8E1gZOAeVt7zDzN4LvB9Vb8uy5EmS9DTp2ZAkSZIkSZIkPYCZvQO4DZgOrBh/RwEXAq+iqm5j3P1nWb0tSZJep6vVKJIkSZIkSZIkEWa2mpldY2Zj4qPlgT+4+wuR0+xalK/hbGBDd3/J3X8GWb0tSZLeJ5UNSZIkSZIkSVID7v4KsAZwQySHfAL4VzM72MzGufuzwN3AX4Bf19jUJEmSymQYRZIkSZIkSZJ0GTMb6e5vxP93AA7sDewKTAKWRYqG41GZ+MfqamuSJEk7pLIhSZIkSZIkSbpIkW/BzMa4+1/iszuAPwOHAG9DiocVgTvd/e76WpskSdIeqWxIkiRJkiRJki5RUjTsCGwGvOruF8d3t6GQif3c/fVCGZHJIJMkWRjJnA1JkiRJkiRJ0iVC0bAzcB7wfeBEM7vYzFZy9w8CywG3x+avF7+pp7VJkiTtk8qGJEmSJEmSJOkCZjbCzJYHDkVhEiOBV4AJwAVmtpy7Twb+CVLJkCTJwk2GUSRJkiRJkiRJFzGz5YBVgGuArYBxwH8CFwMnuPvrNTYvSZKkI4yquwFJkiRJkiRJsqhSytGwBbAhMBd4Dvgrys+wAkoEeT9wSyoakiRZVMgwiiRJkiRJkiQZJkLRsAtwJTAauBqYAfwReBS4HrgL+KK7P1FbQ5MkSTpMejYkSZIkSZIkyTBhZssAHwR2BtYADgRud/dXzewSYFUAd/9Bfa1MkiTpPJmzIUmSJEmSJEmGATPbGuVi2AvYAFgb2Mvdf2FmuwL/6e5P1tjEJEmSYSPDKJIkSZIkSZKkw5jZhsBJKB/Di0jRcGYoGjYDPo8SQyZJkiySpGdDkiRJkiRJknQQM1sZeAy41t1PNrMJwBEoZGIM8C7geHf/eo3NTJIkGVZS2ZAkSZIkSZIkQ8DM1gTWA94PPAP8Dvh74ChgS3f/FzNbCVgJ5W34L3d/pqhUUVe7kyRJhpNUNiRJkiRJkiRJm5jZROAK4HZgaaRsOBg4B9gcmAbs6+4/q62RSZIkNZDKhiRJkiRJkiRpAzN7K3AzcLW7Xx2fvQV4E/gk8ANgNeThMNXd/6WutiZJknSbTBCZJEmSJEmSJO2xEvAtd7/azEaZ2bnAU8B0lADyCOC7wLXAyvU1M0mSpPuMqrsBSZIkSZIkSbKQshXKwQBwEfAKCps4F/grcDnwB3c/o57mJUmS1EcqG5IkSZIkSZKkPV4Exsb/rwMPuvvjZnYg8BAKozDgK/U0L0mSpD4yjCJJkiRJkiRJ2uMlYEczeztwF/BJM9sAGfQeAuYAvzezlLmTJFnsyASRSZIkSZIkSdImZrYn8BZUjeIdwDFI2XA0sCbwYeAod/9TbY1MkiSpgVQ2JEmSJEmSJEmbmNl4YA9gbaRweBkYD6yKPBv2cffnamtgkiRJTaSyIUmSJEmSJEmGgJktBWwGnAj8BngN+B/gYnd/ts62JUmS1EUqG5IkSZIkSZKkA5jZku7+Wvw/0t3fqLtNSZIkdZHJapIkSZIkSZKkM/yl9P+btbUiSZKkB0jPhiRJkiRJkiRJkiRJOkp6NiRJkiRJkiRJkiRJ0lFS2ZAkSZIkSZIkSZIkSUdJZUOSJEmSJEmSJEmSJB0llQ1JkiRJkiRJkiRJknSUVDYkSZIkSZIkSZIkSdJRUtmQJEmSJEmSJEmSJElH+f+8PCPfHgMKPAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_list = freq_dist.most_common(50)\n",
    "\n",
    "x = [tup[0] for tup in freq_list]\n",
    "y = [tup[1] for tup in freq_list]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (18, 6))\n",
    "\n",
    "sns.barplot(\n",
    "    x = x,\n",
    "    y = y,\n",
    "    edgecolor = 'k',\n",
    ")\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = 'right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you would suspect, the time required to complete the processing pipeline we have developed will depend on the type of texts and the number of texts we process. The following code bock shows how we can save the tokenized texts in the same format as the original data (JSON with GZIP compression). After execution, you will have a new file named `processed_tokens.json.gz` in the current directory that contains the processed texts in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Path object for file path\n",
    "processed_tokens_path = pathlib.Path('processed_tokens.json.gz')\n",
    "\n",
    "# \"dump\" the list of tokens to a JSON string\n",
    "json_str = json.dumps(tweet_tokens) + '\\n' \n",
    "\n",
    "# encode the JSON string to bytes\n",
    "json_bytes = json_str.encode('utf-8')\n",
    "\n",
    "# open a GZIP file at the specified path\n",
    "with gzip.GzipFile(processed_tokens_path, 'w') as fout:\n",
    "    \n",
    "    # write the bytes to the file object\n",
    "    fout.write(json_bytes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block generates multiple datasets with different processing pipelines applied. In a future notebook, we will look at the differences in topic models that we find using the various datasets. You can run the cell by changing the value of the `run_cell` variable to `True`. **Note: This cell will run for a while : )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_cell = False\n",
    "sample_size = 50000\n",
    "ngram_threshold = 3\n",
    "\n",
    "if run_cell:\n",
    "    \n",
    "    data_directory = pathlib.Path('data')\n",
    "    if not data_directory.exists():\n",
    "        data_directory.mkdir()\n",
    "    \n",
    "    my_mwe_tuples = [\n",
    "        ('u', '.', 's', '.'),\n",
    "        ('covid', '-', '19'),\n",
    "    ]\n",
    "    my_stopwords = ['..', '...', 'get', '\\u200d', '’', '“', '”']\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Loading raw data')\n",
    "    raw_data_filepath = pathlib.Path('relevant_data.json.gz')\n",
    "\n",
    "    with gzip.GzipFile(raw_data_filepath, 'r') as fp:\n",
    "        json_bytes = fp.read()\n",
    "        json_str = json_bytes.decode('utf-8')\n",
    "        tweet_data = json.loads(json_str)\n",
    "\n",
    "    # convert to Pandas DataFrame\n",
    "    tweet_df = pd.DataFrame(tweet_data, \n",
    "                            columns = column_names)\n",
    "\n",
    "    tweet_df['Datetime'] = pd.to_datetime(tweet_df['Datetime'], \n",
    "                                          format = '%a %b %d %H:%M:%S %z %Y')\n",
    "    tweet_df['Date'] = tweet_df['Datetime'].dt.date\n",
    "    tweet_df['Day_Name'] = tweet_df['Datetime'].dt.day_name()\n",
    "\n",
    "    weekend_mask = tweet_df['Day_Name'].isin(['Saturday', 'Sunday'])\n",
    "    tweet_df.loc[weekend_mask, 'Weekend'] = True\n",
    "    tweet_df.loc[~weekend_mask, 'Weekend'] = True\n",
    "\n",
    "    tweet_df['Hour'] = tweet_df['Datetime'].dt.hour\n",
    "\n",
    "    # get sample\n",
    "    print('Creating sample')\n",
    "    np.random.seed(0)\n",
    "    sample = np.random.choice(tweet_df['Text'].tolist(), \n",
    "                              size = sample_size,\n",
    "                              replace = False,\n",
    "                             )\n",
    "    sample = sample.tolist()\n",
    "    column_names = [\n",
    "        'Datetime',\n",
    "        'Username',\n",
    "        'User',\n",
    "        'Location',\n",
    "        'Language',\n",
    "        'Text',\n",
    "    ]\n",
    "    \n",
    "    for mwe_tuples_list_val in [my_mwe_tuples, None]:\n",
    "        for lemmatize_val in [True, False]:\n",
    "            for stem_val in [True, False]:\n",
    "                for remove_punctuation_val in [True, False]:\n",
    "                    for remove_stopwords_val in [True, False]:\n",
    "                        for include_bigrams in [True, False]:\n",
    "                            for include_trigrams in [True, False]:\n",
    "                                \n",
    "                                data_id = []\n",
    "                                if mwe_tuples_list_val is not None:\n",
    "                                    data_id.append('mwe')\n",
    "                                if lemmatize_val:\n",
    "                                    data_id.append('lemmas')\n",
    "                                if stem_val:\n",
    "                                    data_id.append('stems')\n",
    "                                if remove_punctuation_val:\n",
    "                                    data_id.append('nopunct')\n",
    "                                if remove_stopwords_val:\n",
    "                                    data_id.append('nostopw')\n",
    "                                if include_bigrams:\n",
    "                                    data_id.append('bigram')\n",
    "                                if include_trigrams:\n",
    "                                    data_id.append('trigram')\n",
    "                                if data_id: \n",
    "                                    data_str = '_'.join(data_id)\n",
    "                                else:\n",
    "                                    data_str = 'base'\n",
    "                                data_filepath = pathlib.Path('data', f'{data_str}.json.gz')\n",
    "                                if data_filepath.exists():\n",
    "                                    print(f'Skipping {data_filepath.__str__()}')\n",
    "                                else:\n",
    "                                    print(f'Starting {data_str}')\n",
    "                                    \n",
    "                                    tweet_tokens = prepare_texts(sample, \n",
    "                                                                 mwe_tuples_list = mwe_tuples_list_val,\n",
    "                                                                 custom_stopword_list = my_stopwords,\n",
    "                                                                 lemmatize = lemmatize_val,\n",
    "                                                                 stem = stem_val,\n",
    "                                                                 remove_punctuation = remove_punctuation_val,\n",
    "                                                                 remove_stopwords = remove_stopwords_val,\n",
    "                                                                )\n",
    "\n",
    "\n",
    "                                    if include_trigrams:\n",
    "                                        trigram_finder = nltk.collocations.TrigramCollocationFinder.from_documents(tweet_tokens)\n",
    "                                        trigram_finder.apply_freq_filter(ngram_threshold)\n",
    "                                        trigrams = {trigram: '_'.join(trigram) for trigram in trigram_finder.ngram_fd}\n",
    "                                        for current_index, current_tweet in enumerate(tweet_tokens):\n",
    "                                            tweet_tokens[current_index] = replace_trigrams(current_tweet, trigrams)\n",
    "\n",
    "                                    if include_bigrams:\n",
    "                                        bigram_finder = nltk.collocations.BigramCollocationFinder.from_documents(tweet_tokens)\n",
    "                                        bigram_finder.apply_freq_filter(ngram_threshold)\n",
    "                                        bigrams = {bigram: '_'.join(bigram) for bigram in bigram_finder.ngram_fd}\n",
    "                                        for current_index, current_tweet in enumerate(tweet_tokens):\n",
    "                                            tweet_tokens[current_index] = replace_bigrams(current_tweet, bigrams)\n",
    "                                            \n",
    "                                    json_str = json.dumps(tweet_tokens) + '\\n' \n",
    "                                    json_bytes = json_str.encode('utf-8')\n",
    "                                    with gzip.GzipFile(data_filepath, 'w') as fout:\n",
    "                                        fout.write(json_bytes)    \n",
    "\n",
    "                                    end_time = time.time()\n",
    "                                    elapsed_time = round(end_time - start_time, 2)\n",
    "                                    print(f'Finished (elapsed time is {elapsed_time} seconds)')\n",
    "                                    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
